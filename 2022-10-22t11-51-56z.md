---
date: 2022-10-22 11:51:56
title: "{First/Every} Visit MC Prediction in RL"
id: 2022-10-22t11-51-56z
tags: [uva, uva_rl]
---

Given that [value](./2022-10-21t10-45-34z.md) is defined as the expected return
starting from a given state, a very obvious way of estimating it can be to
simply average sampled returns from a given state.

This is the idea behind _first_ and _every visit Monte Carlo
[prediction](./2022-10-21t19-49-14z.md)_, which estimates $v_\pi(s)$ as the
average of returns following the first visit/all visits to $s$ across the
sampled episodes.

Both approaches converge to the true value as the number of visits goes to
infinity. Each average is unbiased, and the standard deviation falls as
$1/\sqrt{n}$, where $n$ is the number of returns averaged.

Crucially, each estimate is independent of other estimates and other states. In
this sense, these methods do not [bootstrap](./2022-10-22t10-56-02z.md).

The same methods can be used to estimate the
[action-value](./2022-10-21t10-45-34z.md) of each state-action pair. However,
this introduces a much larger search space, which makes it likely for many
state-action pairs to never be visited. We are once again faced with the
[explore / exploit dilemma](./2022-10-20t15-59-00z.md).

There are a few ways of
[addressing the explore / exploit dilemma in MC methods](./2022-10-22t12-19-37z.md)
