---
date: 2021-12-19 17:12:58
title: Lexical Function models for Compositional Distributional Semantics
id: 2021-12-19t17-12-58z
tags: [uva, uva_nlp1]
---

Lexical function models distinguish between two kinds of words:

- words whose meaning is directly determined by their distribution (e.g. nouns).
  I'll refer to these as "units"
- and words that act as "functions", i.e. transform the distribution profile of
  other units (e.g adjectives). I'll refer to these as "lexical functions".

In lexical function models, units are represented by vectors, while lexical
functions are represented by linear transformations, i.e. matrices. Composition
is then matrix multiplication.

As mentioned above, adjectives can be represented as lexical functions using
matrices. The learning of the matrices would consist in the following algorithm:

1. Obtain a distributional vector representation $\mathbf{n}_j$ of each noun
   $n_j$ in the training corpus.
2. Identify adjective-noun pairs $(a_i, n_j)$ in the corpus using a parser.
3. For each pair identified in 2., obtain a distributional vector representation
   $\mathbf{p}_{ij}$ using a distributional semantics model (DSM), for instance.
   For a given adjective $a_i$, the set of tuples
   $\{(\mathbf{n}_j, \mathbf{p}_{ij})\}$ is the dataset $\mathcal{D}(a_i)$ to
   train on
4. The matrix $\mathbf{A}_i$ corresponding to $a_i$ can then be learned via
   linear regression by minimizing the squared error loss:

$$
L(\mathbf{A}_i) = \sum_{j \in \mathcal{D}(a_i)}
||\mathbf{p}_{ij} -\mathbf{A}_i \mathbf{n}_j||^2
$$

Where, as stated, steps 3 and 4 are applied across all adjectives that require
their matrices to be learned.
