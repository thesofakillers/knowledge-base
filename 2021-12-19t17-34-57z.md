---
date: 2021-12-19 17:34:57
title: The Skip-gram model and word2vec
id: 2021-12-19t17-34-57z
tags: [uva, uva_nlp1]
---

The skip-gram model is a [prediction-based](./2021-12-20t18-42-28z.md)
distributional semantics model.

The model consists in a binary logistic classifier whose task is to predict
whether a word $c$ is likely to appear near (as context) to a target word $w$.
We then don't actually care about the prediction, which we discard, and instead
are interested in the trained weight matrices to use as embeddings for the words
in our corpus.

The skip-gram model can be trained in a semi-supervised way, with target and
context words both easily extractable from a corpus by sliding a window.

The general overview of the model can be viewed as follows:

1. Treat the target word and its neighbouring context as positive examples
2. Randomly sample other words in the corpus outside the context and use these
   as negative examples.
3. Use logistic regression to train a classifier to distinguish between these
   two classes
4. Use the learned weights as embeddings.

For classification, the skip-gram makes use of the distributional hypothesis for
computing the probability that a given word $c$ is appears as context to a
target word $w$: if their embedding vectors are similar, then this probability
is computed to be high.

For similarity, a simple dot product $\mathbf{c} \cdot \mathbf{w}$ is used. To
turn this into a probability, the logistic function is used:

$$
P(+ | w, c) = \sigma(\mathbf{c} \cdot \mathbf{w}) =
\frac{1}{1 + \exp{(-\mathbf{c} \cdot \mathbf{w}})}
$$

The probability that $c$ is not a context to $w$ is then just $1 -$ what we just
wrote:

$$
P(- | w, c) = \sigma(\mathbf{c} \cdot \mathbf{w}) =
\frac{1}{1 + \exp{(\mathbf{c} \cdot \mathbf{w}})}.
$$

These are the probabilities for a single pair. The skip-gram model makes the
additional assumption that all context words are independent, allowing us to
multiply probabilities for positive and negative examples for a given target
word

$$
P(+ | w, c_{1:L}) = \prod_i^L\sigma(\mathbf{c_i} \cdot \mathbf{w})
$$

which we can take the logarithm of to express in terms of a sum:

$$
\log P(+ | w, c_{1:L}) = \sum_i^L\log\sigma(\mathbf{c_i} \cdot \mathbf{w}).
$$

The skip-gram model actually learns two weight matrices: the target embeddings
$\mathbf{W}$ and the context embeddings $\mathbf{C}$. After training, it is
common to just add them together, although the context embeddings $\mathbf{C}$
can also be thrown away.

Training the model consists in

- maximizing the similarity of the target word, context word pairs drawn from
  the positive examples
- Minimize the similarity of the pairs from the negative examples

This is done using [SGD](./2021-12-19t22-34-39z.md), maximizing the loss
function as stated below

$$
\arg \max \left(\sum_{\left(w_{j}, w_{k}\right) \in D_{+}}
\log \left(\frac{1}{1+e^{-w_{k} \cdot w_{j}}}\right)
+\sum_{\left(w_{j}, w_{k}\right) \in D_{-}}
\log \left(\frac{1}{1+e^{w_{k} \cdot w_{j}}}\right)\right)
$$

Where $D_+$ and $D_-$ represent the sets of positive and negative examples
respectively.

---

It should be noted that the above is known as skip-gram with negative sampling
(SGNS). The original skip-gram model treats the problem as a multi-class
problem. As such, the softmax involved needs to takes the entire vocabulary as
input. The softmax function involves the summation of an exponentiation, which
for a large vocabulary (derived from a large text corpus), can be quite costly
in terms of computation.

In skip-gram with negative sampling, for each positive sample, we randomly
select $k$ words from the corpus that do not appear in the context of the target
word
