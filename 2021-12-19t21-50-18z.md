---
date: 2021-12-19 21:50:18
title: Sigmoid and Tanh activation functions
id: 2021-12-19t21-50-18z
tags: [uva, uva_dl1]
---

The sigmoid and tanh are two similar
[activation functions](./2021-04-26t15-11-38z.md). Mathematically we have the
sigmoid:

$$
h(x) = \frac{1}{1+\exp(-x)}
$$

And the tanh:

$$
h(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}
$$

Tanh is centered about 0, with output in the range $[-1, 1]$, while sigmoid is
centred around 0.5, with output in the range $[0, 1]$. As such, tanh tends to
have stronger gradients and less "positive" bias for next layers. Sigmoid on the
other hand is suitable for operations making use of gates, such as in
[LSTMs](./2021-04-22t14-59-37z.md)

Both activation functions easily saturate at the extremes of their ranges, which
can cause issues with
[exploding and vanishing gradients](./2021-04-23t16-08-52z.md) and overconfident
networks.
