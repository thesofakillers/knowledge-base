---
date: 2021-04-22 14:59:37
title: Long Short-Term Memory (LSTM)
id: 2021-04-22t14-59-37z
tags: [uva, uva_dl1]
---

A Long Short-Term Memory (LSTM) is a type of
[Recurrent Neural Network](./2020-09-04t13-57-25z.md) that is capable to learn
long-term dependencies through the use of specialized cells
[@hochreiterLongShortTermMemory1997c].

In principle, any RNN can learn long-term dependencies. However, with vanilla
RNNs this is very difficult to achieve due to the issue of
[vanishing/exploding gradients](./2021-04-23t16-08-52z.md)

LSTMs curtail this issue by implementing an augmented version of a constant
error carousel (CEC) in their individual units.

Vanilla CECs prevent vanishing/exploding gradients by enforcing constant error
flow through a given recurrent unit. These architectures however struggle with
conflicting input and output weight updates.

The augmented CECs of an LSTM, referred to as _memory cells_ solve this by
introducing "gate" units. These are multiplicative units that can either "open"
or "close" to accordingly let information enter or exit.

Central to LSTMs and RNNs in general is the notion of the _cell state_,
essentially information that runs across the network sequentially. The gates are
trained to selectively add or remove information from the cell state.

In particular, we have 3 kinds of gates:

- an input gate: regulates which values will be updated
- an output gate: regulates what to send to the next memory cell, to be used to
  update the cell state at that timestep
- a forget gate, introduced to the LSTM architecture in a later paper
  [@gersLearningForgetContinual2000] : removes previously-learned information
  that is no longer relevant

![An LSTM memory
cell (middle)](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)

---

- [Great blogpost explaining LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [Useful StackExchange Post](https://datascience.stackexchange.com/questions/19196/forget-layer-in-a-recurrent-neural-network-rnn)

## References
