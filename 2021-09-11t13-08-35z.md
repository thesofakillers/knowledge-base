---
date: 2021-09-11 13:08:35
title: Bayesian interpretation of probability theory
id: 2021-09-11t13-08-35z
---

In [probability theory](./2021-09-09t11-36-51z.md), it is often useful to
generalize our reasoning in terms of our knowledge and uncertainty around a
given event rather than observations of repeated samplings. This is particularly
useful in the case of events that are difficult to repeat.

The way [Bayes' theorem](./2021-09-10t18-29-28z.md) is set up allows us to
reason in this way, a Bayesian way of reasoning.

- For a given hypothesis or model parametrized by $\mathbf{w}$, we can capture
  our assumptions in the form of a _prior_ distribution $p(\mathbf{w})$.
- We can then capture the _effect_ of the data $D = {t_1, \dots, t_N}$, with the
  conditional probability $p(D|\mathbf{w})$. This is also known as the
  _likelihood function_, representing how probable the observed data set is for
  the given combination of the parameter vector $\mathbf{w}$.
- These two pieces can then be used to update our assumptions by calculating our
  _posterior_ distribution $p(\mathbf{w}|D)$ by applying Bayes' Theorem

$$
p(\mathbf{w}|D) = \frac{p(D|\mathbf{w})p(\mathbf{w})}{p(D)}
$$

Our posterior can allow us to evaluate our uncertainty in $\mathbf{w}$ after
observing $D$.

Bayes' theorem can therefore be expressed in words as such:

$$
\textrm{posterior} \propto \textrm{likelihood} \times \textrm{prior},
$$

with the normalizing denominator term implicit.
