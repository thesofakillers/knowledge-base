---
date: 2021-12-21 18:51:05
title: L1 regularization
id: 2021-12-21t18-51-05z
tags: [uva, uva_dl1]
---

L1 regularization is a [regularization](./2021-12-21t18-46-39z.md) method which
modifies the [loss function](./2021-04-26t19-23-41z.md) by adding a term equal
to sum of the weights being learned, i.e. the L1 norm of the weights.

This has the effect of "encouraging" the weights towards 0, hence preventing the
model from learning overly large (read: confident) weights. Furthermore, L1
regularization often encourages sparsity of the learned weights.

Mathematically, we modify our loss function to include a second term:

$$
w* = \arg \min_w \sum_{i=0}^N\mathcal{L}(y_i, \hat{y}(x_i, w)) +
\frac{\lambda}{2}\sum_{l=0}^M |w_l|
$$

Where $\lambda$ is some scaling term.

L1 regularization is also referred to as "lasso regression".
