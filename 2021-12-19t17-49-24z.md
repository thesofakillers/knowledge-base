---
date: 2021-12-19 17:49:24
title: Sentence and Document Representations in NLP
id: 2021-12-19t17-49-24z
tags: [uva, uva_nlp1]
---

In [NLP](./2021-12-20t10-52-27z.md), it may be desired to obtain document
representations. These can be useful for a variety of tasks:

- text categorization (e.g. by topic)
- sentiment analysis
- authorship attribution
- spam and phishing email filtering
- misinformation detection

To obtain document representations, we first need sentence representations, just
how we had word representations in
[distributional semantics](./2021-12-19t16-47-22z.md). This is because feeding
the whole (long) document to an LSTM word by word will probably just cause the
LSTM to run into the [vanishing gradient problem](./2021-04-23t16-08-52z.md).

We can use [Compositional semantics](./2021-12-19t17-00-58z.md) for our sentence
representations, for instance by combining the hidden states of an LSTM in some
ways:

- using the final hidden state
- using an average of the LSTM hidden states
- using the maximum value in each vector component of all hidden states
- using an attention mechanism, a weighted sum of the hidden states at all time
  steps

Once we have sentence representations, we can also consider document
representations. We build a hierarchical model:

- first compute the sentence representations
- combine the sentence representations into a single document representation
  using another LSTM and/or an attention mechanism over the sentences
- this is also known as a hierarchical attention network (HAN)
