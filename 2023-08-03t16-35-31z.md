---
date: 2023-08-03 16:35:31
title: Thesis Storyline
id: 2023-08-03t16-35-31z
tags: [msc_thesis]
bibliography:
  - literature-notes/references.bib
csl:
  - literature-notes/institute-of-mathematical-statistics.csl
reference-section-title: References
author: Giulio Starace
---

Out-of-distribution (OOD) generalization, the ability to perform well on data
not distributed identically to the training data, is a key challenge in the
field of Machine Learning (ML) [@arjovsky_out_2020]. Recent advances in the
field have led to increasingly urgent discussions on the associated safety
implications
[@hendrycks_unsolved_2022;@ngo_alignment_2022;@hendrycks_overview_2023;@critch_tasra_2023].
In this context, Goal Misgeneralization (GMG)
[@langosco_goal_2022;@shah_goal_2022] is a form of OOD generalization failure
often introduced when discussing potential safety-critical failure modes of ML
models.

GMG occurs when a model trained to pursue a given goal during training,
misgeneralizes during testing and capably pursues a different goal instead. GMG
is of particular concern due to the retained capability of the model when
generalizing, allowing for the possibility of visiting arbitrarily undesirable
states.

We frame GMG as a consequence of causal confusion [@de_haan_causal_2019], where
the underlying causal model for achieving our goal is spuriously correlated with
some other variable during training time, which the model confusingly learns
instead. We identify three possible solutions to the problem:

1. Performing interventions on the data, to better discover the underlying
   causal model.
2. Increasing the variety of the training data, to reduce the chance of spurious
   correlations.
3. Improve task specification, to reduce ambiguity and similarly reduce the
   chance of spurious correlations.

While we expect a combination of these solutions to be most effective, we choose
to focus on this solution: improving task specification. We focus on this
direction because we intuit that most examples of goal misgeneralization are due
to the limited nature by which tasks are specified. It is perhaps unfair to
blame the model for its causal confusion when the bandwidth for specifying the
task is limited to e.g. a one-dimensional signal such as reward in Reinforcement
Learning [@vamplew_scalar_2022].

We focus on the problem area of of sequential decision making (SDM) and choose
to leverage recent advancements in natural language processing, leading us to
language-informed sequential decision making (LISDM). We choose this direction
because we view task specification as a _communication_ between task requester
and task executor. For communicative intents, natural language provides the most
expressive potential.

Here, we require language representations to be "understood" by our policy, what
is also referred to as _grounding_. We make use of a vision-language multimodal
foundation model, CLIP, as our starting point and study a potential modification
route for it to be used as a hypothetical foundation model for SDM.

Ultimately, we present the following contributions:

1. We connect GMG to the phenomenon of Causal Confusion
2. We provide a high level description of the possible solutions to GMG
3. We show how an existing vision-language foundation model can be modified to
   simulate a hypothetical foundation model for SDM.
4. We show how such a hypothetical foundation model could be leveraged for
   improved task specification for the case of addressing GMG.
