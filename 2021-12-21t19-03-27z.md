---
date: 2021-12-21 19:03:27
title: Dropout in Deep Learning
id: 2021-12-21t19-03-27z
tags: [uva, uva_dl1]
---

Dropout is the go-to solution for addressing the issue of
[co-adaptation](./2021-12-26t22-43-41z.md) in
[Deep Neural Networks](./2021-04-26t18-14-48z.md).

With Dropout, at each training batch a random set of neurons in the neural
network are set to an activation of 0. The neurons to be "dropped out" are
typically chosen from a Bernoulli distribution with $p=0.5$.

This "breaks-up" neurons that would otherwise activate in highly-correlated
ways, eliminating the redundancy that was causing issues in co-adaptation. In
this way, each neuron is more "independent", granting the model greater
expressivity while also eliminating reinforced (overly confident) gradients from
co-adapted neurons. The general effect is
[regularization](./2021-12-21t18-46-39z.md), with overfitting being reduced and
improved generalization.

At test time, all neurons are used, with the activations re-weighted by $p$.

Interestingly, Dropout can be thought of as training a slightly different
architecture for each batch, with the model used at test-time acting as an
average of the ensemble.
