---
date: 2022-10-21 15:27:32
title: The Bellman Optimality Equations
id: 2022-10-21t15-27-32z
tags: [uva, uva_rl]
---

When dealing with [optimal policies](./2022-10-21t12-52-19z.md) in
[RL](./2022-10-20t15-15-55z.md), the
[Bellman equations](./2022-10-21t12-59-49z.md) can be written in a special form,
purely in terms of [value functions](./2022-10-21t10-45-34z.md), i.e. without
any reference to a particular [policy](./2022-10-21t10-19-39z.md). These are
called the _Bellman optimality equations_, and can be derived as such:

$$
\begin{align}
 v_*(s) &= \max_a q_{\pi_*}(s, a) \\
        &= \max_a \mathbb{E}_{\pi_*} \left[G_t | S_t = s, A_t = a\right] \\
        &= \max_a \mathbb{E}_{\pi_*} \left[R_{t+1} + \gamma G_{t+1} |
        S_t = s, A_t = a\right] \\
        &= \max_a \sum_{s', r} p(s', r|s, a) \left[ r + \gamma v_*(s') \right].
\end{align}
$$

A similar equation can be derived for the optimal
[action-value](./2022-10-21t11-09-20z.md) function:

$$
\begin{align}
q_*(s,a) &= \mathbb{E} \left[R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') |
            S_t = s, A_t = a\right] \\
         &= \sum_{s', r} p(s', r|s, a) \left[ r +
            \gamma \max_{a'} q_*(s', a') \right].
\end{align}
$$

For finite [MDPs](./2022-10-21t12-12-18z.md), the Bellman optimality equations
have a unique solution for $v_*$ or $q_*$. Once these are obtained, it is
trivial to determine an optimal policy.

However, finding the unique solution is akin to an exhaustive search, and is as
such rarely possible in practice.
