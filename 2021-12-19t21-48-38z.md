---
date: 2021-12-19 21:48:38
title: Leaky ReLU
id: 2021-12-19t21-48-38z
tags: [uva, uva_dl1]
---

The Leaky Rectified Linear Unit (Leaky ReLU) is a modification of the
[ReLU](./2021-12-19t21-47-16z.md) activation function that allows a small
positive gradient when the unit is not active. This is avoid the issue of dead
neurons. Mathematically it is defined as

$$
h(x)=\left\{\begin{array}{c}
x, \text { when } x>0 \\
a x, \text { when } x \leq 0
\end{array}\right.
$$

Where $a$ is some parameter that can be either be manually set or learned. In
the latter case, we refer to the activation function as _Parametric ReLU_ or
_PReLU_.
