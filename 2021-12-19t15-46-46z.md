---
date: 2021-12-19 15:46:46
title: Part of Speech Tagging using Hidden Markov Models
id: 2021-12-19t15-46-46z
tags: [uva, uva_nlp1]
---

[POS tagging](./2021-12-20t12-16-56z.md) can often be addressed using
[HMM](./2021-12-19t15-49-14z.md). Here, our observations are the words and our
hidden states are the POS tags, since these are typically not explicitly stated
in text. Given a labeled corpus, we can compute transition and emission
probabilities by counting.

For transition probabilities we have:

$$
P(t_i | t_{i-1}) = \frac{C(t_{i-1}, t_i)}{C(t_{i-1})},
$$

And for emission probabilities we have:

$$
P(w_i | t_i) = \frac{C(t_i, w_i)}{C(t_i)}.
$$

The process of **decoding** then consists in finding the most probable sequence
of states $Q$ given our HMM and a sequence of observations $O$. Mathematically
for our tagging problem:

$$
\hat{t}_{1:n} = \underset{t_1, \dots, t_n}{\text{argmax}}
\: P(t_1, \dots, t_n | w_1, \dots, w_n).
$$

Using [Bayes' Theorem](./2021-09-10t18-29-28z.md) and dropping the denominator
we can re-express this as

$$
\hat{t}_{1:n} = \underset{t_1, \dots, t_n}{\text{argmax}}
\: P(w_1, \dots, w_n | t_1, \dots, t_n)P(t_1, \dots, t_n).
$$

To this we apply the HMM independence assumptions allowing us to express our
probabilities as products:

$$
P(w_1, \dots, w_n | t_1, \dots, t_n) \approx \prod_{i=1}^n P(w_i | t_i),
$$

And

$$
P(t_1, \dots, t_n) \approx \prod_{i=1}^n P(t_i | t_{i-1}),
$$

Leaving us with

$$
\hat{t}_{1:n} \approx \underset{t_1, \dots, t_n}{\text{argmax}} \:
\prod_{i=1}^n P(w_i | t_i)P(t_i | t_{i-1})
$$
