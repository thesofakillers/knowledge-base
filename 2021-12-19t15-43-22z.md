---
date: 2021-12-19 15:43:22
title: The Perplexity Metric
id: 2021-12-19t15-43-22z
tags: [uva, uva_nlp1]
---

In information theory, perplexity is a measure of how well a probability
distribution or model predicts a sample. It is often found in
[natural language processing](./2021-12-20t10-52-27z.md) for the evaluation of a
[language model](./2021-12-20t11-06-56z.md).

Intuitively, a language model that assigns a high probability to a test set can
be said to "not be suprised" by the test set, or in other words that the model
is "not perplexed" by the test set.

From this intuition, we can define perplexity to be the inverse probability of
the test set. To account for test sets of different sizes $N$, we normalize the
probability by taking the $N$th square root (see
[here](https://towardsdatascience.com/perplexity-in-language-models-87a196019a94)
for simple derivation).

This leaves us with the following mathematical definition of perplexity $PP$ of
a test set $W$:

$$
PP(W) = \frac{1}{P(w_1, w_2, \dots, w_N)^{\frac{1}{N}}} = P(w_1, w_2, \dots,
w_N)^{-\frac{1}{N}}
$$

In general, the lower the perplexity, the better our Language Model
