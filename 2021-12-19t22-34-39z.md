---
date: 2021-12-19 22:34:39
title: Stochastic Gradient Descent
id: 2021-12-19t22-34-39z
tags: [uva, uva_ml1]
---

Normal [gradient descent](./2021-04-27t16-51-42z.md) requires evaluating the
model on every example in the entire dataset to compute the model's expected
likelihood. This can be very computationally demanding, particularly for large
datasets or for complicated models.

However, it can been shown that we can compute these expectations by randomly
sampling a small number of examples (a minibatch) from the dataset, then taking
the average over only those examples.

This is known as minibatch gradient descent, which is more commonly referred to
as stochastic gradient descent (SGD). This method is much more efficient, as we
essentially gain new iterations (equivalent to our previous full-dataset
iterations) for free.

A common mathematical expression for the SGD update is

$$
w^{(t)} = w^{(t-1)} - \eta g^(t)
$$

where $w^{(t)}$ is the parameter we are updating, $\eta$ is the learning rate
and $g^(t)$ is the gradient we estimated.
