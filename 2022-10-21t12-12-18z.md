---
date: 2022-10-21 12:12:18
title: (Finite) Markov Decision Processes
id: 2022-10-21t12-12-18z
---

_Markov Decision Processes_ (MDP) is _the_ framework used for formalizing the
problem of sequential decision making, typically studied in the field of
[RL](./2022-10-20t15-15-55z.md). An MDP consists in the following elements:

- The _agent_: the entity that learns and makes decisions
- The _environment_: the entity that the agent interacts with; everything other
  than the agent
- _rewards_: numerical values emitted by the environment that the agent tries to
  maximize through its choice of actions.

More specifically, the agent and environment interact continuously, over a
sequence of discrete time steps $t = 0, 1, 2, \dots$. At each time step, the
environment emits a representation of itself, the
[state](./2022-10-21t10-20-25z.md), which the agent receives and uses to decide
which action to take. At the next time step, the agent receives a reward.
Repetitions of this form what is known as a _trajectory_, which looks like

$$
S_0, A_0, R_1, S_1, A_1, R_2, S_2, \dots
$$

In _finite_ MDPs, the state, action and rewards spaces are finite. Crucially,
the result is that the random variables $R_t$ and $S_t$ have well defined
discrete [probability distributions](./2021-09-10t19-20-02z), depending _only_
on the previous state and action:

$$
p(s', r | s, a) \dot{=} \text{Pr} \left{S_t = s', R_t = r | S_{t-1} = s, A_{t-1} = a\right}.
$$

This probability distribution defines the _dynamics_ of the MDP. This
restriction on the state, that it must include all the information necessary to
predict the next state, is what makes the MDP a _Markov_. It is the _Markov_
property of the states in an MDP.

This Markov property is the same assumption that defines a
[Markov chain](./2021-12-20t11-29-13z.md).
