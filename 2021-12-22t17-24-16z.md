---
date: 2021-12-22 17:24:16
title: Training Latent Variable Models and Inference
id: 2021-12-22t17-24-16z
tags: [uva, uva_dl1]
---

Like most probabilistic models, we train
[latent variable models](./2021-12-22t16-35-13z.md) by
[maximizing the log likelihood](./2021-09-11t15-54-04z.md):

$$
\theta_{ML} = \arg \max_\theta \sum_{i=1}^N \log p_\theta(x_i)
$$

However, this does not have a [closed-form](./2020-10-09t13-35-53z.md) solution
for latent variable models, so iterative algorithms are used instead. Computing
the [gradient](./2021-04-27t18-05-20z.md) of the log-likelihood for a single
data point, we see that we need to compute the posterior distribution $p(z|x)$:

$$
\begin{aligned}
\nabla_{\theta} \log p_{\theta}(\mathbf{x})
&=\frac{\nabla_{\theta}p_{\theta}(\mathbf{x})}{p_{\theta}(\mathbf{x})}
  =\frac{\int \nabla_{\theta} p_{\theta}(\mathbf{x},\mathbf{z})d\mathbf{z}}
  {p_{\theta}(\mathbf{x})} \\
&=\frac{\int p_{\theta}(\mathbf{x},\mathbf{z})
  \nabla_{\theta} \log p_{\theta}(\mathbf{x},\mathbf{z})
    d\mathbf{z}}{p_{\theta}(\mathbf{x})} \\
&=\int p_{\theta}(\mathbf{z} \mid \mathbf{x})
  \nabla_{\theta} \log p_{\theta}(\mathbf{x}, \mathbf{z}) d \mathbf{z}
\end{aligned}.
$$

The process of computing the posterior distribution over our latent variable $z$
is referred to as _inference_. Mathematically, we can express interest as

$$
p(z|x) = \frac{p(x, z)}{p(x)} = \frac{p(x, z)}{\int p(x, z) dz}.
$$

We see that this requires computing the marginal likelihood of the observation,
which is typically intractable analytically. Instead, we use
[variational inference](./2021-12-22t18-12-57z.md)
