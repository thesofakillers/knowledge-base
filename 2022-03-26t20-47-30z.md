---
date: 2022-03-26 20:47:30
title: Smoothing for Query Likelihood in IR
id: 2022-03-26t20-47-30z
tags: [uva, uva_ir1]
---

The default [query likelihood](./2022-03-26t20-46-43z.md) approach is too naive.
There are two issues:

1. if any of the query words are missing from the document, the score given by
   the query likelihood model for $P(Q|D)$ will be zero.
2. We will also not be able to distinguish between documents that have different
   numbers of query words missing.

To account for this issue, which stems from data sparsity we make use of
Jelinek-Mercer smoothing, which adjusts query likelihood as such:

$$
P(Q|D) = \prod_{i=1}^n\left((1-\lambda)\frac{f_{q_i, D}}{|D|} +
\frac{c_{q_i}}{|C|}\right),
$$

where $\lambda$ is some constant, $c_{q_i}$ is the count of times $q_i$ occurs
in the collection of documents $C$.

As we [often do](./2021-09-11t16-23-45z.md), we take the log to simplify
computation:

$$
P(Q|D) = \sum_{i=1}^n\log\left((1-\lambda)\frac{f_{q_i, D}}{|D|} +
\frac{c_{q_i}}{|C|}\right).
$$

There is also
[Dirichlet Smoothing](http://mlwiki.org/index.php/Smoothing_for_Language_Models#Dirichlet_Prior_Smoothing),
as an alternative.
