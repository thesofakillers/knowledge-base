---
date: 2023-07-10 18:45:27
title: "Sanity Check: Extrinsically evaluating CLIPT embeddings with Task
  Classification"
id: 2023-07-10t18-45-27z
tags: [msc_thesis]
---

We perform a quick sanity check on the performance of the
[CLIPT](./2023-07-10t16-36-37z.md) trajectories by using these as input to a
simple to classifier trained to classify the trajectories as belonging to
particular tasks. We can do this because each language-annotated trajectory in
the dataset is also annotated with the task it belongs to. It is trained
exclusively on the cross-entropy loss of classifying 34 tasks based on
**visual** trajectory representations from CLIPT. It’s then evaluated on **both
visual and textual** trajectory representations.

We find the performance gap here to be promising, suggesting that our CLIPT
embeddings are of relatively high quality for what we want to do.

![training curves](task-classifier_curves.png)

Surprisingly, the textual trajectories perform better on the test set:

![test set performance](task-classifier_test.png)

One possible explanation is that the frames and/or trajectories in the test set
are somewhat out of distribution compared to those in the train split, while the
textual descriptions do not really change too much between splits. You can sort
of see this also by the fact that textual accuracy stays basically unchanged
between validation and test (~0.85) while visual accuracy drops from 0.89 in
validation to 0.70 in test.

I’m still somewhat surprised because this is the D -> D task, where both
training and test frames come from the same setup.
