---
date: 2022-10-22 18:16:54
title: Expected SARSA
id: 2022-10-22t18-16-54z
tags: [uva, uva_rl]
---

_Expected SARSA_ is a [temporal difference](./2022-10-22t10-55-03z.md) control
algorithm which extends [SARSA](./2022-10-22t18-03-29z.md) by taking into
account how likely each next action is. It does this by using the
[expected value](./2021-09-11t12-11-20z.md) of the next action instead of the
value of a randomly sampled action. Mathematically, the update becomes

$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha
          \left[
    R_{t+1} + \gamma \sum_{a'} \pi(a'|S_{t+1}) Q(S_{t+1}, a') - Q(S_t, A_t)
    \right].
$$

Note that the expectation can be calculated under any policy, even one different
from the behaviour policy, whose sole responsibility is sample where to update,
making expected SARSA an [off-policy RL algorithm](./2022-10-22t12-29-58z.md).

However note that expected SARSA does not need to rely on [importance
sampling](./2022-10-22t13-16-08z.md). [Why?](./2022-10-22t18-58-14z.md)

While more computationally complex than SARSA, Expected SARSA is eliminates the
variance introduced by the random sampling of the next action that occurs in
SARSA.

Note that Expected SARSA generalises [Q-learning](./2022-10-22t18-42-24z.md),
which is a special case where the target policy is greedy.
