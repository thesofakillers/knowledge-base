---
date: 2022-10-22 13:16:08
title: Importance Sampling for Off-Policy RL
id: 2022-10-22t13-16-08z
tags: [uva, uva_rl]
---

Importance sampling is a technique for estimating
[expected values](./2021-09-11t12-11-20z.md) under one
[distribution](./2021-09-10t19-20-02z.md) given samples from another.

This is necessary in [off-policy learning](./2022-10-22t12-29-58z.md) because we
wish to estimate the expected [returns](./2022-10-21t11-04-35z.md) under the
target policy, but only have samples obtained under the behaviour policy.

By making use of an _importance sampling ratio_, we can transform the estimated
expected returns under the behaviour policy into an estimate of the expected
returns under the target policy.

The importance sampling ratio is defined as

$$
\rho_{t:T-1} = \prod_{k=t}^{T-1} \frac{\pi(a_k | s_k)}{b(a_k | s_k)}.
$$

There are two ways in which we can use the importance sampling ratio to estimate
$v_\pi$:

- [ordinary importance sampling](./2022-10-22t16-31-02z.md)
- [weighted importance sampling](./2022-10-22t16-31-13z.md)
