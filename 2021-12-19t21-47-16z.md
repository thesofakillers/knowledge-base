---
date: 2021-12-19 21:47:16
title: The Rectified Linear Unit (ReLU)
id: 2021-12-19t21-47-16z
tags: [uva, uva_dl1]
---

The Rectified Linear Unit (ReLU) is one of the most commonly used
[activation functions](./2021-04-26t15-11-38z.md) in
[Neural Networks](./2021-04-26t18-14-48z.md). Mathematically, it is defined as:

$$
h(x) = \max(0, x).
$$

ReLU provides a number of desirable properties:

- in a randomly initialized network, only around 50% of units are activated with
  this activation function. This is known as **sparse representation**, which is
  a desirable properties for NNs to have, as it can accelerate learning while
  regularizing (simplifying) the model
  - sparsity is not only more biologically plausible
  - it can also offer mathematical advantages (see
    [Deep Sparse Rectifier Neural Networks](https://proceedings.mlr.press/v15/glorot11a.html)
    for more details):
    - information disentangling
    - Efficient variable-size representation
    - Linear separability
    - Distributed but sparse
- **better gradient propagation**: fewer vanishing/exploding gradient problems
- **efficient computation**: only comparison, addition and multiplication
  necessary
- **scale-invariant**

The activation function is however not perfect:

- it is non-differentiable at zero, which means we either have to hard-code a
  fix or we will get errors when gradient computations require evaluation at 0.
- it is not zero-centred, which means we need to adjust our weight
  initialization process.
- it is unbounded, which could cause exploding gradient issues
- it can cause _dead neurons_, neurons that are pushed into states of
  non-activation for all inputs.
