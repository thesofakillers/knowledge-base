---
date: 2022-10-22 17:47:51
title: The TD error in RL and TD(0)
id: 2022-10-22t17-47-51z
tags: [uva, uva_rl]
---

To exploit consistency of [value](./2022-10-21t10-45-34z.md),
[temporal difference learning](./2022-10-22t10-55-03z.md), TD learning methods
use the next timestep as the [target](./2022-10-22t17-36-28z.md) of their
updates. In other words, the TD update rule looks like:

$$
V(S_t) \leftarrow V(S_t) + \alpha \left[R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\right].
$$

The target is now $R_{t+1} + \gamma V(S_{t+1})$ instead of $G_t$, and we refer
to the error term in brackets as the _TD error_.

When using this target, we refer to the algorithm as _TD(0)_ or _one-step TD_.
