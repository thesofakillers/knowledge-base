---
date: 2022-07-04 16:29:21
title: Encoder-only vs Decoder-only vs Encoder-Decoder Transformers
id: 2022-07-04t16-29-21z
---

Since the seminal
[_Attention is All You Need_](https://arxiv.org/abs/1706.03762) paper
[@vaswani_attention_2017] outlining the Transformer architecture, three main
variants have established themselves:

1. encoder-decoder transformers: Composed of an encoder and decoder block linked
   by encoder-decoder attention.
   - The original transformer made use of this architecture.
   - these models are well suited for tasks where we want to generate new text
     in some way related to the input, for instance summarization or question
     answering
2. encoder-only transformers: These are transformers solely composed of the
   encoder block.
   - BERT [@devlin_bert_2019] is an example.
   - these models are well suited for when we want to work with sentence-level
     representations, _classification_ and/or contextual embedding.
     - this is reflected in the bidirectional training where the masked language
       modeling (MSM) objective is used.
3. decoder-only transformers: These are transformers solely composed of the
   decoder block
   - GPT [@radford_language_2019; @brown_language_2020] is an example.
   - these models are well suited for
     [language modeling](./2021-12-20t11-06-56z.md) or generally where
     open-ended text _generation_ is desired
     - this is reflected in the unidirectional training where language modeling
       is the training objective: predict all words to the right of a given
       sentence

## References
