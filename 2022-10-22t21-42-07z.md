---
date: 2022-10-22 21:42:07
title: Additional notes on RL
id: 2022-10-22t21-42-07z
tags: [uva, uva_rl]
---

All the notes linked directly or indirectly to [RL](./2022-10-20t15-15-55z.md)
so far dealt with algorithms that requiring _storing_ estimated
[state](./2022-10-21t10-45-34z.md) or [action values](./2022-10-21t11-09-20z.md)
in tables. For this reason, we refer to these methods as _tabular methods_.

Tabular methods present a series of limitations:

1. Storing a table is only feasible for discrete, finite and 'small' sets of
   actions and states.
2. Our stochasticity is pre-set. There is no nuance to _how_ stochastic our
   learning should be under particular conditions.
3. We are very sample inefficient: we have sample many times before convergence,
   with no possibility for planning ahead.

Luckily these limitations can be addressed through a variety of alternatives.
Limitations 1 and 2 can for instance be addressed by using _approximation_.
Limitation 3 can be addressed by using _model-based_ methods, which we will come
back to later.

## Approximation

Approximation methods aim to represent the value function in a compact way,
allowing for generalisation to nearby states while still being close to the true
value function. This is done by parametrising the function with some parameter
$w$. There are a variety of ways to approximate the value function, both
linearly and non-linearly. Linear methods include polynomials,
aggregation/tiling, radial basis functions and others.
[Neural networks](./2021-04-26t18-14-48z.md) are a popular non-linear method.

To aim towards being close to the true value function, approximation methods aim
to minimize the value error:

$$
\overline{\text{VE}} = \sum_s \mu(s) [v_\pi(s) - \hat{v}(s, \mathbf{w})]^2
$$

where $\mu$ is some weighting factor based on how important a given state is.

We then make our update in the direction of the negative gradient, essentially
performing [stochastic gradient descent](./2021-12-19t22-34-39z.md). Before we
can compute the gradient however, we need a way of approximating $v_\pi$. From
here there are a variety of routes.

In gradient MC, we use the return $G$. This allows to to take the gradient and
gives us the gradient Monte Carlo update:

$$
\mathbf{w}_{t+1} = \mathbf{w}_t + \alpha \left[G_t - \hat{v}(s,
      \mathbf{w}_t)\right] \nabla \hat{v}(s, \mathbf{w}_t).
$$

Gradient MC always converges to the minimum of the value error.

An alternative to gradient MC is semi-gradient TD. Here we use the bootstrapping
estimate $R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w}_t)$ instead of the return
and cut off a term from the full gradient (as this has been found to work
better). Unlike gradient MC, semi-gradient TD converges to the TD fixed-point,
when using linear features. When using non-linear features, this not necessarily
the case and can in fact diverge. While the TD fixed point is at best
$1/(1-\gamma)$ worse than the best possible error, semi-gradient TD has been
found to converge faster than gradient MC, and so in certain problems can
outperform.

Given the result of the TD fixed point, we can take a shortcut and directly
estimate the components of the TD fixed point from data rather than converging
to it iteratively. This is the reasoning behind Least-Squares TD. This is the
most data efficient form of TD(0), but also the most computationally expensive.
LSTD circumvents the need for a step-size hyperparameter, however it is not
widely adopted because along with its computational complexity, it also presents
the issue that it does not forget, meaning that it cannot be used with
[non-stationary](./2022-10-21t11-57-08z.md) systems.

So far, these approximation methods consider the on-policy case. What about
[off-policy](./2022-10-22t12-29-58z.md)? This requires the use of the
[importance sampling ratio](./2022-10-22t13-16-08z.md). However, with importance
sampling, semi-gradient TD has been found to diverge.

This leads to the "deadly triad" of desires which we cannot give up but are
difficult to reconcile:

- function approximation: without it we can't scale to continuous, infinite
  and/or large state and action spaces
- bootstrapping: without it we must rely on the much slower MC
- off-policy learning: without it we cannot learn multiple policies at the same
  time and/or re-use data

To address this, we return to the [bellman equation](./2022-10-21t12-59-49z.md)
and from there define the _bellman error_, a measure of how far an approximation
$v_\mathbf{w}$ is from the true value function $v_\pi$. The _bellman operator_
is then defined as the operator that when applied to an approximation
$v_\mathbf{w}$ represented in the representable subspace, produces a new
function that is outside the subspace, the distance between these two points
being the _bellman error vector_, i.e. the vector of all bellman errors at all
states. Repeated applications of the bellman operator will eventually produce
the true value function, but this is generally impossible, since this will lay
outside the representable subspace. Therefore the result of the application of
the bellman operator needs to be projected back into the representable subspace,
obtaining the projected bellman error, known as PBE. At the TD fixed point, the
PBE is 0.

We can try to minimize many different errors. Minimizing the mean squared TD
error produces weird results, while minimizing the bellman error is not possible
from data only. Our final option is minimizing PBE so to find the TD fixed point
off-policy. This is done in the GTD2 algorithm, a gradient TD method which
converges to 0 mean squared PBE when using linear features, and converges to a
local optimum when using non-linear features. The GTD2 algorithm requires a
separate update (and therefore step-size) for the final term, therefore causing
the need to store and update 2 parameter vectors.

What about Q-learning? That was off-policy. Is there an approximate version?
Yes, that is Deep Q Networks, DQN, which use a semi-gradient version of
Q-learning. Here we use a neural network to approximate the state-action value
function. DQN apply the same preprocessing to all input: scaling, grayscaling
and stacking of images. These images are then fed through a series of
[convolutional layers](./2021-12-21t22-19-52z.md) interleaved with the
[ReLU](./2021-12-19t21-47-16z.md) before being fed to a fully connected layer
with one unit for each action. DQN's additionally use experience replay to break
correlations between transitions. This is done by storing experiences in a
buffer and training on a batch of the experiences at each time step. This
ensures the network doesn't overfit to the most recent experience. A final
trick, DQNs clip the error between -1 and 1, which helps to stabilize training.

## Policy-based Methods

So far, all of these methods have been [value-based](./2022-10-22t10-34-46z.md).
However, these methods struggle with handling continuous action spaces, as it is
difficult to apply the max operator here. Furthermore, these methods have no way
of governing the smoothness of the discovered policy, no way of including prior
knowledge and no way of elegantly learning stochastic policies. Instead of
optimizing a value function and extracting a policy from it, we can instead
directly optimize a policy function.

One way of finding the best policy is using Policy Gradients. Classical
REINFORCE does this by calculating the analytical gradient. However, this has
high variance. We can subtract a baseline such as the value function to reduce
the variance. G(PO)MDP is what most people refer to when they refer to
REINFORCE. This algorithm extends classical REINFORCE by using an expanding
window over time-steps for computing estimates, cutting one source of variance.

There are more algorithms, like PGT (which is an actor-critic method), DPG, NPG,
and TRPO. I will not cover these in these notes.

## Model-based RL

So far, all of our methods have been _model-free_. This means that they do not
rely on a model of the environment to learn. However, what if we design a model
of the environment and use it to help us with learning? Then we are dealing with
_model-based_ RL. In general, any process that uses a model rather than real
data to obtain a policy is referred to as "planning". Model-based approaches can
provide many solutions to the problems of model-free approaches, such as
expensive, risky or time-consuming data, no access to the dynamics, no
counterfactuals or gradients and the impossibility of on-policy learning for
certain problems.

We refer to the learned simulators of the environment as _dynamics models_.
These can be

- distribution-based: where a complete description of transition probabilities
  and rewards can be given
- sample-based: where sampled rewards and new states can be generated
- trajectory-based: where an episode can be simulated, but jumping to an
  arbitrary state cannot be done (OpenAI gym environments for example).

While model-based methods have lower sample complexity, unlike model-free
methods they can be affected by modelling errors. Model-based and model-free
learning can also be combined. Dyna and Dyna-Q are examples of this.

## Partially Observability

So far we have assumed that that the agent has access to the true state of the
environment. However, in the real world, this is rarely the case. When modelling
partially observable MDPs, the information we get about the state arrives in the
form of _observations_. These observations can be seen as features of some
latent state.

A useful heuristic is that there is information in the history of interactions.
An optimal policy should be able to exploit all of the information available in
the history. However, working with full histories is annoying. We can instead
work with _features_ of histories, which we would like to possess two
properties:

1. It should be _compact_: low dimensional.
2. It should capture all the relevant information.

Two features of histories are considered to be equivalent if they predict the
same future. We have also have the notion of a _markov state_ and _internal
state_ (aka _belief state_).

## Multi-Agent RL

Multi-agent RL (MARL) is the study of RL problems considering more than one
agent simultaneously interacting with the same environment and potentially with
each other.

MARL faces several of the same challenges as normal RL, but often to a higher
degree. Some of the challenges faces are instead unique to MARL.

- The curse of dimensionality: more agents -> more dimensions.
- Local optima: more agents -> more local optima to get stuck in.
- Non-stationarity: even in stationary environments, the policies of other
  agents may change, ultimately causing dynamics to look non-stationary from the
  perspective of a given agent.
- credit assignment: in a multi-agent environment, it is not always clear which
  agent is responsible for a given outcome.

MARL often relies on the field of _Game Theory_ to help with addressing the
challenges. Game Theory studies the interactions of "rational" agents with
potentially conflicting goals.

MARL typically deals with two types of games from game thoery:

- Fully cooperative games
  - An agent has imperfect information, so we formalise using Decentralised
    POMDPs.
  - in general, it is difficult to select an appropriate nash equilibrium
- Two-player zero sum
  - Here, max-min strategy is the preferred solution typically: strategy that
    maximizes worst-case payoff.
  - may need a mixed strategy

MARL has also been approached using Deep Learning, in what is known as Deep MARL
(DMARL). Here often we have "independent learning", where we run a separate RL
algorithm for each agent. If the agents are all identical, we can share
parameters.

Note that while in deployment the policies are executed in a decentralised way,
during training, the process can be centralised. One way of doing this is with a
_centralised critic_. This is a critic, in the actor-critic sense, i.e. a value
function, that is shared by all agents. This helps with non-stationarity, but is
generally quite complex and difficult to learn.

Finally, how do we evaluate MARL? One way, for zero-sum games, is to generate
Joint Policy Correlation (JPC) matrices. Here we train policies with several
different random seeds and then pair policies with different seeds to see if any
overfitting to specific partners/opponents occurs.
