---
date: 2022-04-11 10:31:19
title: The T0 Language Model
id: 2022-04-11t10-31-19z
---

T0/T0+/T0++ (”tee-zero (plus (plus))”) [@sanhMultitaskPromptedTraining2022] is a
zero-shot capable general [language model](./2021-12-20t11-06-56z.md) (LM) based
on the 11B parameter architecture of the T5 model
[@raffelExploringLimitsTransfer2020]. Unlike other zero-shot LM's, T0 achieves
generalization via _explicit_ training on a variety of tasks (multitask
learning). This is made possible by re-structuring a variety of supervised
learning datasets into human-readable prompted form via specialized tool known
as PromptSource [@bachPromptSourceIntegratedDevelopment2022]. Despite being 16x
smaller, T0 outperforms GPT-3 [@brownLanguageModelsAre2020] in several tasks.

## References
