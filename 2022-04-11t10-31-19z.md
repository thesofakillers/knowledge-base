---
date: 2022-04-11 10:31:19
title: The T0 Language Model
id: 2022-04-11t10-31-19z
---

T0/T0+/T0++ ("tee-zero (plus (plus))") [@sanhMultitaskPromptedTraining2022] is a
zero-shot capable general [language model](./2021-12-20t11-06-56z.md) (LM) based
on the 11B parameter architecture of the [T5 model](./2022-05-12t15-31-59z.md)
[@raffelExploringLimitsTransfer2020]. Unlike other zero-shot LM's, T0 achieves
generalization via _explicit_ training on a variety of tasks (multitask
learning). This is made possible by re-structuring a variety of supervised
learning datasets into human-readable prompted form via specialized tool known
as PromptSource [@bachPromptSourceIntegratedDevelopment2022].

Despite being 16x smaller, T0 outperforms GPT-3 [@brownLanguageModelsAre2020] in
generalization to several held-out tasks.

It should be noted that T5, in this case, actually refers to T5+LM, Lester et
al. (2021)'s LM-adapted T5 model [@lesterPowerScaleParameterEfficient2021].

The authors also experiment with robustness to prompt formulation. They find
that training on more prompts per dataset leads to better and more robust
generalization to held-out tasks. They also find non-original task prompts can
also be beneficial. In general, the authors find that T0 is more robust to
prompt formulation than GPT-3.

## References
