---
date: 2021-12-22 15:06:55
title: Wasserstein GANs
id: 2021-12-22t15-06-55z
tags: [uva, uva_dl1]
---

In their original formulation [GANs](./2021-12-22t11-42-44z.md) made use of JS
divergence to measure how close the generated and input distributions were.

This metric is fine for overlapping distributions, but what happens when our
distributions do not overlap (as is the case most of the time)?

It can be shown that JS divergence is always $\log 2$ for two non-overlapping
distributions. This is problematic as distributions not overlapping by the same
amount will be treated equally. In other words, there is no notion of distance
or extent in the lack of overlap. The result of this is that if our two
distributions do not overlap, our discriminator will achieve 100% accuracy,
making it impossible for our generator to perform further learning.

Wasserstein GANs address this issue by integrating wasserstein distance between
our distributions $P_G$ and $P_{data}$. This essentially adds a Lipschitz
constraint to our discriminator objective, which ensures smoothness. In this
way, non-overlapping distributions are not equally treated, with the resulting
divergence being weighted by the wasserstein distance instead.
