---
date: 2022-10-21 11:48:49
title: Upper-Confidence Bound in RL
id: 2022-10-21t11-48-49z
tags: [uva, uva_rl]
---

For addressing the [explore / exploit dilemma](./2022-10-21t11-48-49z.md) in
[RL](./2022-10-20t15-15-55z.md), we can use the _Upper-Confidence Bound_ (UCB)
as an alternative to [epsilon-greedy](./2022-10-21t11-33-20z.md) action
selection.

The intuition behind UCB is that in epsilon-greedy action selection, when
exploring, we do not make an informed choice about which actions to choose,
instead choosing randomly. It would perhaps be better to explore actions that
have a higher chance to be optimal.

UCB does this by choosing actions both on the estimated
[q value](./2022-10-21t11-09-20z.md) _and_ on the uncertainty associated with
this estimate. Mathematically, UCB chooses actions with

$$
A_t \dot{=} \arg\max_a \left[ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right],
$$

where $c$ is a hyperparameter governing the extent to which we explore, and
$N_t(a)$ is the number of times action $a$ has been chosen up to time $t$.

UCB is not a very flexible method, with difficulties encountered for instance
with [nonstationary](./2022-10-21t11-57-08z.md) problems and problems dealing
with large state spaces.
