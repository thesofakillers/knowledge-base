---
date: 2022-03-26 20:46:19
title: Language Models for Term-based Retrieval
id: 2022-03-26t20-46-19z
tags: [uva, uva_ir1]
---

In [term-based retrieval](./2022-03-26t20-44-53z.md), we can use
[language models](./2021-12-20t11-06-56z.md) to represent our queries and
documents as vectors.

How we approach this depends on the kind of language model we use. For example,
for a unigram language model $M$ modeling some corpus $d$, the probability of a
sequence of terms $t_1, \dots, t_N$ is simply

$$
p(t_1, \dots, t_N | M_d) = \prod^N_{i=1} p(t_i)
$$

Where the probability of given term is just

$$
p(t_i | M_d) = \frac{tf(t_i, d)}{len(d)}.
$$

For a bigram language model, we instead have

$$
p(t_1, \dots, t_N | M_d) = p(t_1) p(t_2 | t_1) p(t_3 | t_1) \dots p(t_{N}
| t_{N-1}).
$$

Because we now work with probabilities, in term-based retrieval, LM's can be
used for ranking via [Query Likelihood](./2022-03-26t20-46-43z.md).

They can also be used for ranking via
[KL-Divergence Matching](./2022-03-26t20-47-03z.md)
