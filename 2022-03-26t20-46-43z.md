---
date: 2022-03-26 20:46:43
title: IR matching with Query Likelihood
id: 2022-03-26t20-46-43z
tags: [uva, uva_ir1]
---

In query likelihood, we rank documents based on the probability of that the
query text could be generated by the document
[language model](./2022-03-26t20-46-19z.md).

Essentially we are after $P(D|Q)$, which using
[Bayes' Rule](./2021-09-10t18-29-28z.md) we can express as

$$
P(D|Q) = \frac{P(Q|D)P(D)}{P(Q)}.
$$

We can ignore the normalizing constant as we only care about _rank_ equivalence

$$
P(D|Q) = P(Q|D)P(D).
$$

In most cases, we assume the prior to be uniform (the same for all documents),
thus not affecting the ranking. We are thus left with

$$
P(D|Q) = P(Q|D) .
$$

Using a uniform language model, we can compute this with

$$
P(D|Q) = P(Q|D) = \prod^n_{i=1} p(q_i | D) = \prod^n_{i=1}\frac{f_{q_i, D}}{|D|}
$$

where $q_i$ is a term in the query, $f$ is its frequency, and there are $n$
terms in the query. This is a [maximum likelihood](./2021-09-11t15-54-04z.md)
estimation.

We note that when using term-frequency to estimate probability we will have 0
probabilities for terms not present in the vocabulary. This issue is addressed
via [smoothing](./2022-03-26t20-47-30z.md).
