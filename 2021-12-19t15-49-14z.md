---
date: 2021-12-19 15:49:14
title: Hidden Markov Models
id: 2021-12-19t15-49-14z
tags: [uva, uva_nlp1]
---

A [markov chain](./2021-12-20t11-29-13z.md) can be directly applied when the
events being modeled can be directly observed. However, we often are interested
in events that are not directly observed, i.e. "hidden". From a sequence of
observable states, we are interested in _inferring_ a sequence of hidden states.

A Hidden Markov Model (HMM) allows us to consider observed events (observations)
and hidden events (states) in a single model, achieving exactly what was
presented at the end of the first paragraph.

Like markov chains, HMM make use of the **markov assumption**: the probability
of a particular state depends only on the previous state. HMM then also make an
additional assumption on **output independence**: the probability of an output
observation $o_i$ depends exclusively on the state $q_i$ that produced the
observation:

$$
P(o_i | q_1, \dots, q_T, o_i, \dots, o_T) = P(o_i | q_i)
$$

Apart from these assumptions, a HMM is specified by the following components:

- A set $Q=q_i, \dots, q_N$ of $N$ **states**
- A **transition probability matrix** $\mathbf{A}$, with each element $a_{ij}$
  representing the probability of moving from state $i$ to state $j$, with
  $\sum_{j=1} a_{ij} = 1 \forall i$
- A sequence $O=o_1, \dots, o_T$ of $T$ **observations** drawn from a set of
  possible observations $V$
- A sequence $B$ of observation likelihoods also called **emission
  probabilities**, each expressing the probability of $o_t$ being generated from
  a state $q_i$
- An **initial probability distribution** over states, expressing the
  probability that a given state is the initial state in the Markov Chain.
