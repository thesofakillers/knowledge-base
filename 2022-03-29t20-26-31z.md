---
date: 2022-03-29 20:26:31
title: Latent Dirichlet Allocation
id: 2022-03-29t20-26-31z
tags: [uva, uva_ir1]
---

Latent Dirichlet Allocation (LDA) is a method for the
[semantic retrieval](./2022-03-26t20-48-08z.md) of documents given a query.

To account for vocabulary mismatch, where words in a query are missing from
relevant documents leading to 0 probabilities in
[term-based retrieval](./2022-03-26t20-44-53z.md), LDA smooths the probabilities
of unseen words by considering their relation to the _topic_ of the given
document.

LDA does this by modeling documents as _mixtures of topics_, with a topic simply
being treated as another [language model](./2021-12-20t11-06-56z.md), as
[previously seen](./2022-03-26t20-46-19z.md). The assumption is that for a given
collection of documents, there is a fixed number of underlying (or
[latent](./2021-12-22t16-35-13z.md)) topics.

The LDA process for generating a document is then

1. For each document $D$, pick a multinomial distribution $\theta_D$ from a
   Dirichlet distribution with parameter $\alpha$.
2. for each word position in the document $D$:
   1. pick a topic $z$ from $\theta_D$
   2. choose a word $w$ from $P(w | z, \beta)$, a multinomial probability
      conditioned on the topic $z$ with parameter $\beta$.

Once these distributions are available, we can produce a language model for the
words:

$$
P_{lda}(w|D) = P(w|\theta_D, \beta) = \sum_z P(w|z, \beta) P(z|\theta_D).
$$

We can then integrate this language model into our
[smoothed query-likelihood model](./2022-03-26t20-47-30z.md), obtaining the
following Frankenstein:

$$
p(w|D) = \lambda \left( \frac{f_{w, D}\mu \frac{c_w}{|C|}}{|D| + \mu}\right)+
(1-\lambda)P_{lda}(w|D).
$$
