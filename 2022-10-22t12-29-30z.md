---
date: 2022-10-22 12:29:30
title: ε-soft policies in RL
id: 2022-10-22t12-29-30z
tags: [uva, uva_rl]
---

ε-soft policies are [policies](./2022-10-21t10-19-39z.md) where

$$
\pi(a | s) > \frac{\epsilon}{|\mathcal{A}|} \quad \forall a, s.
$$

When using ε-soft policies, through the
[policy improvement theorem](./2022-10-21t17-34-21z.md), we can guarantee that
our policies can be improved to an ε-greedy policy. This improvement is enough
to apply [generalised policy iteration](./2022-10-21t19-05-11z.md), converging
to the best possible policy among all ε-soft policies.

Using ε-soft policies, we can partly address the
[exploration-exploitation dilemma](./2022-10-20t15-59-00z.md), sacrificing some
performance (our converged policy is not [optimal](./2022-10-21t12-52-19z.md))
while guaranteeing a good exploration of the state-action pair space.
