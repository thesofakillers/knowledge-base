---
date: 2023-04-17 11:14:27
title: Small Survey on Vision-Language Encoders as of April 2023
id: 2023-04-17t11-14-27z
---

CLIP [@radford_learning_2021] is the go-to model in the field as of today. More
than a model, CLIP is a method for training two models: a text encoder and
vision encoder. The models are trained such that their representations are in
similar semantic spaces, such that the text encoder's embedding of the word
"apple" is similar to the visual encoder's embedding of an image of an apple.
CLIP achieves this through _contrastive_ learning of image-caption pairs: After
collecting a dataset of image-caption pairs, the authors train the two encoders
on a shared contrastive loss. For a given image-caption pair, the loss seeks to
maximize the similarity between the embeddings, while minimizing the similarity
with other captions for the image and other images for the caption in the pair.
CLIP uses a [decoder-only](./2022-07-04t16-29-21z.md) transformer as its textual
encoder[^1] and either a ResNet [@he_deep_2016] or a visual transformer (ViT)
[@dosovitskiy_image_2022] for its visual encoder.

ALIGN [@jia_scaling_2021] is very similar to CLIP, but instead of carefully
curating their dataset, with well paired images and captions, they leverage a
noisy data of images and alt-text descriptions, relying on the scaling laws to
achieve results, demonstrating that expert knowledge is not necessary for
dataset curation in contrastive learning.

BEIT-3 [@wang_image_2022] and M3AE [@geng_multimodal_2023] take a different
approach: instead of training an encoder for each modality and ensuring that the
representations from each are similar, they train a single encoder capable of
handling both modalities. In particular, their setup essentially consists in
performing masked language modeling (MLM) with images and text at the same time.
This avoids needing paired data and allows larger scale training on images, text
and image-text data on a single architecture.

FLAVA [@singh_flava_2022] is in a similar vein, utilizing a single model for
multimodal learning. It however also includes cross-modal contrastive
objectives, aiming for performance on image-only, text-only and image-text
downstream tasks with the same single model.

[^1]:
    It is pretty strange that they use a decoder architecture for encoding, but
    hey it works. See
    [this question on stackexchange](https://ai.stackexchange.com/q/39073/55107).

## References
