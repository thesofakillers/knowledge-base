---
date: 2021-12-21 19:39:24
title: Hyperparameters, Parameters and Features in Machine Learning
id: 2021-12-21t19-39-24z
---

In [ML](./2021-09-09t10-48-40z.md), one distinguishes between **features**,
**parameters** and **hyperparameters**:

- **features**: These are the data points that we use to describe the phenomenon
  or problem we are trying to model.
  - For example in linear regression y = ax + b, x is the feature.
  - The process of selecting and forming the best features for our problem is
    [**feature engineering**](https://en.wikipedia.org/wiki/Feature_engineering)
    - One could omit feature engineering entirely in very
      [Deep Neural Nets](./2021-04-26t18-14-48z.md) depending on the problem and
      just use the raw data as the features and let extended training and the
      network capacity extract the features implicitly and model the problem.
      - This was the case in the world of
        [Computer Vision](./2021-09-25t17-12-58z.md) where many state of art
        results were achieved by abandoning feature engineered approaches
        (e.g.[HoG - SVM](https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients))
        in favour of
        [Convolutional Neural Nets](https://en.wikipedia.org/wiki/Convolutional_neural_network)
        where the raw pixels were directly fed into the algorithm to exploit the
        greater compute power and capacity to achieve more complex models.
- **parameters**: These are the variables/coefficients that the algorithm tunes
  so to achieve lower loss or find a minimum, etc.
  - For example in a linear regression y = ax + b, a and b are the parameters.
    In neural nets, the [weights and biases](./2021-04-26t15-11-38z.md) in the
    hidden layers are the parameters.
  - We don't touch the parameters ourselves as the algorithm (be it backprop,
    monte-carlo or what have you) takes care of them for us.
- [**hyperparameters**](<https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)>):
  These are variables governing the actual algorithm;
  - for example: the learning rate, the number of layers, the stride, the number
    of neurons in a particular layer, temperature in simulated annealing etc..
  - We can perform
    [**hyperparameter optimization**](https://en.wikipedia.org/wiki/Hyperparameter_optimization)
    to find the best set of hyperparameters either manually by guessing and
    checking in a greedy manner or automatically through dedicated algorithms.
    - hyperparameter optimization, can of course, indirectly affect the
      parameters themselves.
