---
date: 2021-09-11 17:38:41
title: Maximum a Posteriori
id: 2021-09-11t17-38-41z
---

Given a dataset $D = x, \dots, x_N$, we are given the objective to find the
model parametrized by $\mathbf{w}$ that best explains the data.

What this boils down to is finding the write configuration of $\mathbf{w}$.

In [MLE](./2021-09-11t15-54-04z.md), we do this by maximizing the likelihood
function $p(D|\mathbf{w})$.

Maximum a Posteriori on the other hand achieves this by maximizing the
_posterior_ function:

$$
\mathbf{w}_{MAP} = \textrm{argmax}[p(\mathbf{w}|D)].
$$

What this means is choosing the most probable $\mathbf{w}$ given the data.

The posterior can be obtained via [Bayes' rule](./2021-09-10t18-29-28z.md).

$$
\begin{align}
\mathbf{w}_{MAP}  &= \textrm{argmax}_w[p(\mathbf{w}|D)]\\
                  &=\textrm{argmax}_w \frac{p(D|\mathbf{w}) p(\mathbf{w})}{p(D)}
\end{align}
$$

Like in MLE, we can work in terms of the logarithm of the posterior, for ease of
analysis and computation.

$$
\begin{align}
\mathbf{w}_{MAP}  &= \textrm{argmax}_w \frac{
                        p(D|\mathbf{w}) p(\mathbf{w})
                        }{p(D)}\\
                  &= \textrm{argmax}_w[p(D|\mathbf{w}) p(\mathbf{w})]\\
                  &= \textrm{argmin}_w[-\log p(D|\mathbf{w})-\log p(\mathbf{w})].
\end{align}
$$

Often we condition our prior $p(\mathbf{w})$ with a certainty parameter
$\alpha$, writing it as $p(\mathbf{w}|\alpha)$. This is known as a
_hyperparameter_.
