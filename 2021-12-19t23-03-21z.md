---
date: 2021-12-19 23:03:21
title: AdaGrad for SGD
id: 2021-12-19t23-03-21z
tags: [uva, uva_dl1]
---

This is a gradient update algorithm extending [SGD](./2021-12-19t22-34-39z.md)
such that the learning rate is adaptive. This is done by dividing the learning
rate by a squared sum of the gradient. The result is that the learning rate
decreases as gradient increases and viceversa.

$$
w_{t+1} = w_t - \frac{\eta}{\sum_t{(\nabla_w\mathcal{L})^2} + \epsilon} g_t
$$
