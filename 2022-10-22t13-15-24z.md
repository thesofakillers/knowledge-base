---
date: 2022-10-22 13:15:24
title: Coverage in Off-Policy RL
id: 2022-10-22t13-15-24z
tags: [uva, uva_rl]
---

In [off-policy learning](./2022-10-22t12-29-58z.md), to be able to use a
behaviour policy $b$ to generate the experience to learn a target policy $\pi$,
we assume that for any action taken by the target policy, this same action needs
to be taken, at least occasionally, by the behaviour policy. That is

$$
\pi(a | s) > 0 \implies b(a | s) > 0 \quad \forall s, a.
$$

This is known as _coverage_. A consequence of coverage is that the behaviour
policy must be stochastic in states where it is not identical to the target
policy, while the target policy may be deterministic.
