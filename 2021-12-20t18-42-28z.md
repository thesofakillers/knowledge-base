---
date: 2021-12-20 18:42:28
title: Prediction-based Models in Distributional Semantics
id: 2021-12-20t18-42-28z
tags: [uva, uva_nlp1]
---

In the original presentation of
[distributional semantics](./2021-12-19t16-47-22z.md), vectors were computed by
counting the co-occurrences of a given word with other words, so to define its
context.

The resulting vectors contain explicit interpretable information on the word
usage in context and due to the sparse nature of language, were quite sparse and
long.

It has been shown [@baroni_dont_2014] that training a model to
_predict_ plausible contexts for a given word, learning the word representations
in the process, leads to much better results.

In this re-framing, the vectors are dense, with latent dimensions. These provide
a number of advantages:

- easier to use features for [ML](./2021-09-09t10-48-40z.md) models
- better generalization
- can capture synonymity better with latent features

These dense vectors are typically referred to as "word embeddings"

A very notable prediction-based model is the
[skip-gram model](./2021-12-19t17-34-57z.md) of
[word2vec](https://en.wikipedia.org/wiki/Word2vec).
