---
date: 2021-04-26 19:23:41
title: Learning in the context of ANNs and the Cost Function
id: 2021-04-26t19-23-41z
---

In the context of [ANNs](./2021-04-26t18-14-48z.md), learning refers to the
process of finding the right combination of
[weights and biases](./2021-04-26t18-54-00z.md)

But how does the network know what the "right" combination is? This is usually
achieved by specifying a _cost function_. This is a function which takes the
network's current weights and biases as input and outputs a single number
describing the network's current performance with respect to its objective over
several training examples.

In general, the lower the value of the cost function, the better the network is
performing. "Learning" in this context therefore reduces to _minimizing the cost
function_.

This is typically accomplished with
[stochastic gradient descent](./2021-04-27t16-51-42z.md), with the gradient of
the cost function being computed with an algorithm known as
[backpropagation](./2021-04-27t16-55-26z.md).

Because learning is tied to cost function minimization, and this minimization is
essentially obtained through vector calculus, this requires our cost function to
be smooth. This is part of why artificial neurons hold a continuous value
(between 0 and 1) for their activation, as opposed to actual neurons which are
exclusively either active (1) or inactive (0).
