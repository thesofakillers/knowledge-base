---
date: 2022-10-21 18:56:54
title: Value Iteration in RL
id: 2022-10-21t18-56-54z
tags: [uva, uva_rl]
---

A drawback of [policy iteration](./2022-10-21t18-51-34z.md) is that each
iteration includes a complete run of
[iterative policy evaluation](./2022-10-21t17-23-28z.md), which can require
multiple sweeps through the state space and only converges to the true value
asymptotically.

Value iteration addresses this issue by truncating the iterative policy
evaluation after a single sweep through the state space, i.e. after a single
update of each state. This is done by using the
[Bellman optimality equation](./2022-10-21t15-27-32z.md) as an update rule.

The value is iteratively updated through successive applications of the bellman
optimality update. Once convergence is reached, the optimal policy is extracted
by greedily selecting the action that maximizes the value function.
