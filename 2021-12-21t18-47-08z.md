---
date: 2021-12-21 18:47:08
title: L2 Regularization
id: 2021-12-21t18-47-08z
tags: [uva, uva_dl1]
---

L2 regularization is a [regularization](./2021-12-21t18-46-39z.md) method which
modifies the [loss function](./2021-04-26t19-23-41z.md) by adding a second term
consisting in the sum of the squared weights being learned, i.e. the L2 norm of
the weights.

This has the effect of "encouraging" the weights towards 0, hence preventing the
model from learning overly large (read: confident) weights.

Mathematically, we modify our loss function to include a second term:

$$
w* = \arg \min_w \sum_{i=0}^N\mathcal{L}(y_i, \hat{y}(x_i, w)) +
\frac{\lambda}{2}\sum_{l=0}^M w_l^2
$$

Where $\lambda$ is some scaling term.

L2 regularization is also referred to as "ridge regression" or "weight decay".
