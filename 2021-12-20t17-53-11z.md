---
date: 2021-12-20 17:53:11
title: Context in Distributional Semantics
id: 2021-12-20t17-53-11z
tags: [uva, uva_nlp1]
---

Central to the field of [Distributional Semantics](./2021-12-19t16-47-22z.md) is
the notion of context.

As designers of our models, we can define different kinds of contexts

- **Word windows**: for a given word in a piece of text, we define its context
  as the $n$ words appearing before and after the given word.
  - these can be **unfiltered** or **filtered**, where we ignore (filter out)
    specific kinds of words via a stop-list (a manually compiled list) or via
    POS tags, for instance filtering out closed-class words.
- **Lexeme windows**: The same as word windows, with the difference that instead
  of using the words as they appear in the text, we use their stemmed version.
  - Just like word windows, these can be filtered or unfiltered
- **Dependencies**: for a given word in a piece of text, we define its context
  as the dependency structure it belongs to, as parsed by a dependency grammar.

Furthermore, the context can then be weighted using
[pointwise mutual information](./2021-12-19t16-50-54z.md)
