---
date: 2021-04-26 15:11:38
title: Neurons, Activations, Weights and Biases in ANNs
id: 2021-04-26t15-11-38z
---

In ANNs, a "neuron" or "unit" (the latter nomenclature used to distinguish it
from a biological neuron) is one of the basic building blocks of the neural
network.

In particular, it serves to contain a number, the unit's "activation".
Activations are used in combination with the activations stored in the other
units of the network to compute output activations given input activations.

Different inputs will lead to different activations (different units containing
the highest numbers) and hence different outputs.

But does an ANN 'know' which units to activate given an input vector?

Well, for a given unit, each connection is associated with a given weight. The
given unit's activation is then equal to the weighted sum of all of the unit's
input activations -- the activations of the units connected to (not from) the
unit.

Very often, this weighted sum is passed into a non-linear function (AKA
activation function) that maps the output to a desired range, such as [0, 1] as
is achieved with the
[sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function).

It is also quite common to add a bias to the output of the weighted sum before
passing it through the activation function.

So we can summarize the activation vector of a given layer $\mathbf{a}^{(1)}$
with

$$
\mathbf{a}^{(1)} = \sigma (\mathbf{W}\mathbf{a}^{(0)}  + \mathbf{b}),
$$

where $\mathbf{a}^{(0)}$ is the activation vector of the previous layer,
$\mathbf{W}$ is the weight matrix and $\mathbf{b}$ is the bias vector.

The ANN is then left with the task of [learning](./2021-04-26t18-54-00z.md) the
appropriate combination of weights and biases.
