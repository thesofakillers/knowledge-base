---
date: 2023-07-10 15:06:42
title: High-level description of the approach in my thesis
id: 2023-07-10t15-06-42z
tags: [msc_thesis]
---

For my [thesis](./2023-07-10t14-32-02z.md), we seek to address
[goal misgeneralization](./2023-07-10t14-38-57z.md) in
[sequential decision making](./2023-04-11t15-08-41z.md) by focusing on the third
[possible solution](./2023-07-10t15-08-32z.md) that one may consider: task
specification.

We focus on this direction because we intuit that most examples of goal
misgeneralization are due to the limited nature by which tasks are specified. It
is perhaps unfair to blame the model for its causal confusion when the bandwidth
for specifying the task is limited to e.g. a one-dimensional signal such as
reward in [RL](./2022-10-20t15-15-55z.md).

To do this, we choose to leverage recent advancements in
[natural language processing](./2021-12-20t10-52-27z.md), leading us to
[language-informed sequential decision making](./2023-04-11t15-01-36z.md). We
choose this direction because we view task specification as a _communication_
between task requester and task executor. For communicative intents, natural
language provides the most expressive potential.

To be able to use pretrained [language models](./2021-12-20t11-06-56z.md), we
need their representations to be [grounded](./2023-04-17t11-15-58z.md) to the
[environment](./2022-10-21t12-12-18z.md) of our task. To do this, we make use of
[vision-language multimodal models](./2023-04-17t15-31-08z.md), such as CLIP.

In particular, we
[fine-tune CLIP to handle _trajectories_ instead of single images (CLIPT)](./2023-07-10t16-36-37z.md).
We then train a [[behavioural cloning]] policy conditioned on visual goal
trajectory representations from CLIPT. Because we used CLIPT, at inference time,
we should be able to use textual goal trajectories instead, effectively
utilizing language to specify our tasks.
