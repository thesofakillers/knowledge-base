---
date: 2021-12-22 19:08:10
title: Amortized Variational Inference
id: 2021-12-22t19-08-10z
tags: [uva, uva_dl1]
---

In classical [variational inference](./2021-12-22t18-12-57z.md), we must learn a
different set of variational parameters for each datapoint. This is costly and
not very flexible.

To avoid doing this, amortized variational inference introduces a function (for
instance a [Neural Network](./2021-04-26t18-14-48z.md)) that maps from our
observation space to our variational posterior.

This way we only have to worry about optimizing the parameters of the function,
which are shared across all observations. This provides some advantages:

- the number of parameters to optimize is constant with the size of the dataset
- once optimized, for a new observation, we simply need to pass it through the
  function rather than optimizing again
