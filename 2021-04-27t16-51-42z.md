---
date: 2021-04-27 16:51:42
title: Gradient Descent
id: 2021-04-27t16-51-42z
---

Gradient Descent is an optimization algorithm for finding a local minimum of a
differentiable function. The basic underlying mechanism is iteratively moving
the input of the given function in the direction _opposite_ of the function's
[gradient](./2021-04-27t18-05-20z.md). This is because the gradient of a
function at a given point describes the direction of steepest increase.

In other words, for a given function $f$, we can minimize it's value at a given
point ($\mathbf{p}$) with the following steps comprising gradient descent:

- compute $\nabla f(\mathbf{p})$
- take a small step in the $- \nabla f(\mathbf{p})$ direction
- repeat

The output of $- \nabla f(\mathbf{p})$ is a vector of the same size as
$\mathbf{p}$, describing how much to shift each component of $\mathbf{p}$ so to
decrease $f$ the most effectively.
