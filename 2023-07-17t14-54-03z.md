---
date: 2023-07-17 14:54:03
title: Experimental Setup for showing and addressing Goal Misgeneralization
id: 2023-07-17t14-54-03z
tags: [msc_thesis]
---

In the [high-level description of the approach](2023-07-10t15-06-42z.md) to
[my MSc thesis in AI](2023-07-10t14-32-02z.md), we mostly provide an overview of

1. The problem: [Goal Misgeneralization](2023-07-10t14-38-57z.md) (GMG).
2. The proposed solution: Use language to improve task specification
3. Justification for focusing on this solution
4. How to implement the approach: use multi-modal foundation models and
   behavioural cloning.

That note is missing:

- [[What's the problem with Goal Misgeneralization?]]: addressed by linked note
- How do we experimentally evaluate whether our approach is promising for
  addressing goal misgeneralization?: Addressed below

## Experimental Setup for Goal Misgeneralization

We work with the BabyAI RL environment framework. This framework provides a few
key ingredients:

- **Baby Language**: a combinatorially rich subset of English for expressing
  tasks that can be parsed by the framework's _bot_ and _verifier_
- A **bot**, that given a Baby Language instruction can generate demonstrations
  for that instruction
- A **verifier**, that given a Baby Language instruction and sequence of actions
  can verify whether that sequence of actions satisfies the instruction.

Our setup then consists in the following:

1. We generate a corpus _C_ of text-annotated demonstrations of an agent
   pursuing a number of different tasks in the BabyAI grid environment. We do
   this by
   1. Collecting $T \approx 10^1$ pre-existing or custom made BabyAI tasks $t$
      and their corresponding instructions, $i_t$
   2. Generating additional instructions for each task, not necessarily in Baby
      Language:
      1. For each task $t$ generating $P$ paraphrases for each instruction
         $p_{i_n}$ either with templates or with a language model or some other
         technique.
      2. Adding the generated paraphrases back into the pool of instructions for
         the task
   3. For each instruction $i$, generate $D$ demonstrations, $d_{i_n}$ using the
      BabyAI bot.
   4. Pair the demonstrations with the instructions by joining on the task. We
      can have the same instruction for multiple demonstrations.
2. We train a [CCLIPT](./2023-07-11t11-38-00z.md) model on _C_.
3. Setup a [CoinRun](https://github.com/openai/coinrun)-style environment in
   BabyAI: An agent needs to navigate a gridworld to collect an item under two
   settings:
   - training: the item is always in the same place.
   - testing: the item appears in random places.
4. Generate a corpus _CR_ of annotated demonstrations of the agent successfully
   acting in this environment (training setting only)
5. Train an agent with [CCLIPT](./2023-07-11t11-38-00z.md)-enabled
   [GCBC](./2023-07-11t10-17-09z.md) on _CR_. Refer to this agent as _CeGCBC_.
6. Train an agent with [[Reward-Conditioned behavioural cloning]]. Refer to this
   agent as _RCBC_.
   - Maybe it's simpler to just train an RL agent here
   - For RCBC, could just have an additional term in our loss that maximises
     reward.
   - Am I just describing [[Offline RL]]?
   - We do this because the whole point of what we're doing is improving the
     task specification quality, so we compare to this baseline where the task
     specification is very coarse (a reward) and inexpressive.
7. Evaluate CeGCBC and RCBC on the testing setting (item in random places): The
   RCBC agent _should_ suffer from GMG. Does CeGCBC also suffer from it?

## How are we measuring Goal Misgeneralization?

In our setup we have two clear spuriously correlated variables: The
"bottom-rightedness" of our agent (captured by its (x,y) position) at the end of
training and whether the item was collected. The latter of these is truly
desired, whereas the former is simply a confounding variable. GMG happens if the
behaviour of our agent causes a reward function capturing the completion of the
confounding variable to increase while a reward function capturing the
completion of the desired variable does not. Under this toy environment, both
variables are simple enough that reward functions can be defined for them.

## Additional Considerations

- I'm most uncertain about the baseline we compare to (step 5)
- We also wanted to empirically show the dangers of ambiguity in language:
  perhaps some thought should be devoted to constructing a parallel corpus to
  _CR_, _CRA_, where the demonstrations are the same but the language
  annotations are ambiguous. We could then evaluate with ambiguous and
  unambiguous task specifications.
- Note, with slightly more effort, we could probably come up with a GMG scenario
  different from CoinRun-style ones.

- Starting to think that our GCBC policy needs to be multi-task, or at least
  have a multi-task pre-training phase, to encourage the policy to actually
  "listen" to its conditioning. Otherwise it will just perform behavioural
  cloning and disregard the conditioning[^1]. In steps 3 onwards, the
  environment we train has a single task. We may need to adjust this to consist
  in several tasks, and perhaps keep the coin location fixed throughout. To be
  able to compare to an RL baseline, we'd train a multi-task policy, where the
  task to do is specified with a one-hot encoding.

[^1]:
    I suppose random starting states may serve as "different tasks" to the
    extent that some conditioning may occur, but I'm not sure the variability is
    enough to avoid the problem.
