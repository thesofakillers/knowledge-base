---
date: 2021-12-22 19:22:15
title: Variational Autoencoders
id: 2021-12-22t19-22-15z
tags: [uva, uva_dl1]
---

Variational Autoencoders (VAEs) are an extension to
[autoencoders](./2021-12-22t19-22-32z.md) by re-framing the network as a
[latent variable model](./2021-12-22t16-35-13z.md) trained with
[amortized variational inference](./2021-12-22t19-08-10z.md).

In particular, we re-write our [ELBO](./2021-12-22t18-12-57z.md) as

$$
\mathcal{L}_{\theta, \phi}(x) =
\mathbb{E}_{q_\phi(z)}\left[\log p_\theta(x|z)\right] - KL(q_\phi(z|x)||p_\theta(z))
$$

We now recognize two terms in the RHS respectively:

- the first term is the negative reconstruction loss, measuring how well the
  model reconstructs a sample from the variational posterior
- the second term is a regularization loss, pushing the variational posterior
  towards the prior.

Returning to our extension of the autoencoder, we note that $q_\phi(z|x)$ is
implemented as our encoder network and $p_\theta(x|z)$ is implemented as our
decoder network.

Like any latent variable model, VAEs will suffer with
[variational inference suboptmiality](./2021-12-22t19-44-42z.md).
