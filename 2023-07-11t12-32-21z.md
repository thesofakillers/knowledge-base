---
date: 2023-07-11 12:32:21
title: Is CCLIPT just relying on the shared context between textual and visual
  trajectories?
id: 2023-07-11t12-32-21z
tags: [msc_thesis]
---

In [CCLIPT](./2023-07-11t11-38-00z.md), the first halves of the input to the MLP
encoder heads for textual and visual trajectories are identical: in both cases
we encode the current state and concatenate it with the encoding of the final
state or text depending on the kind of trajectory.

CCLIPT is then trained by vanilla [[constrastive loss]], to reduce the distance
between pairs of related visual and textual trajectories and increase it
otherwise. Because half of the input is shared, perhaps the encoders are simply
learning to ignore the second half of the input, which is where most of the
information is contained. This could be causing the
[large gap in performance when eliminating suggestive starts in GCBC evaluation](./2023-07-11t12-12-49z.md).

To investigate, we perform [intrinsic evaluation](./2023-07-10t18-29-00z.md) of
CCLIPT before and after setting the second half of the inputs to 0. If CCLIPT is
not simply ignoring the second half, we should notice a large drop in
performance. If there is no drop in performance, then this is indicative that
CCLIPT is just ignoring the second half.

Below, we report the results of this investigation

Without the ablation (i.e. without removing the second half of the encoder
inputs (setting that half to 0)), we obtain the following results:

![cclipt no ablation](cclipt_no_ablation.pdf)

```plaintext
Accuracy@1: 0.238
Accuracy@3: 0.457
Accuracy@5: 0.570
Accuracy@10: 0.773
Accuracy@20: 0.906
Accuracy@50: 0.973
```

With the ablation, we obtain the following results

![cclipt with ablation](cclipt_ablation.pdf)

```plaintext
Accuracy@1: 0.016
Accuracy@3: 0.031
Accuracy@5: 0.047
Accuracy@10: 0.082
Accuracy@20: 0.176
Accuracy@50: 0.363
```

We notice a large accuracy drop, which indicates that CCLIPT is not simply
ignoring the second half and memorizing the first half. This is good news, as it
means that CCLIPT is actually learning to encode the visual and textual
trajectories in a way that is useful.

Note however, that the results presented here are for a version of CCLIPT was
trained in two stages, first without the context on the text trajectories (just
as CLIPT), and then with the context on the text trajectories, but with the
visual trajectory encoder frozen. Perhaps this first stage of training where the
current state was entangled with the textual annotation prevented the
memorization from happening. I wonder if when training from scratch in a single
stage, the encoders will indeed learn to ignore the second halves, since after
all they share the first half of their input.
