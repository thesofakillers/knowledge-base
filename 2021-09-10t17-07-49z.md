---
date: 2021-09-10 17:07:49
title: The Sum Rule of Probability
id: 2021-09-10t17-07-49z
tags: [uva, uva_ml1]
---

Recall [joint probability](./2021-09-09t19-46-09z.md) and the variables $X$ and
$Y$ we defined.

We may be interested in knowing the probability that $X$ takes the value $x_i$
irrespective of the value of $Y$. This is written as $p(X = x_i)$.

This will be equivalent to the number of times $X$ is sampled to be $x_i$ out of
the total number of sampling trials $N$ (in which both $X$ and $Y$ are sampled):

$$
p(X = x_i) = \frac{c_i}{N}.
$$

We can write $c_i$ in terms of $n_{ij}$, the total number of times that $X$ and
$Y$ are respectively sampled to be $x_i$ and $y_j$. We do this by summing over
the various configurations of $Y$:

$$
c_i = \sum_{j}n_{ij}.
$$

This will allow us to write $p(X = x_i)$ in terms of the joint probability of
$X$ and $Y$:

$$
\begin{aligned}
p(X = x_i) &= \frac{c_i}{N}\\
p(X = x_i) &= \frac{\sum_{j}n_{ij}}{N}
\end{aligned}
$$

From our definition of joint probability, we know that
$n_{ij} = N p(X=x_i,Y=y_j)$, leaving us with the Sum Rule of Probability:

$$
p(X = x_i) = \sum_{j}^L{p(X=x_i,Y=y_j)}.
$$

This is also known as the _marginal probability_, since it is obtained by
eliminating or marginalizing the other variables via summation.

In more compact form:

$$
p(X) = \sum_Y{p(X,Y)}
$$
