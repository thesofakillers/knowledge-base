---
date: 2023-07-11 10:17:09
title: Goal-Conditioned Behavioural Cloning
id: 2023-07-11t10-17-09z
tags: [msc_thesis]
---

The main [policy](./2022-10-21t10-19-39z.md) implementation in my
[MSc thesis](./2023-07-10t14-32-02z.md) is Goal-Conditioned Behavioural Cloning
(GCBC). This is a policy of the form

$$
\pi(a | s, g)
$$

I.e. the distribution over actions given the current state and goal.

We focus on the [[CALVIN]] dataset to start with, as this consists in a 3D
robotic arm environment which we view as potentially more representative of real
life applications. Here the current state is a concatenation of proprioceptive
state representing the robot joints and a visual representation of the
environment from a static camera, handled by a perception module. We predict
"relative actions" as the [[CALVIN]] authors find better performance when doing
this.

We focus on [[behavioural cloning]] for two reasons:

1. To keep things simple.
2. The dataset we are using is not labeled with rewards.

In principle, the goal $g$ here could be anything, but we treat it in terms of
"goal trajectories" which we represent either visually or textually via
[CLIPT](./2023-07-10t16-36-37z.md). We train on visual trajectory
representations which can easily be sourced from [[play data]], and use CLIPT as
a "bridge" to be able to use textual trajectory representations at test time,
effectively enabling
[language-informed sequential decision making](./2023-04-11t15-01-36z.md),
without explicitly training on language, which may be hard to source.

The training is done by minimizing a
[discretized mixture of logits loss](https://www.giuliostarace.com/posts/dlml-tutorial).
