---
date: 2021-12-22 11:42:44
title: Generative Adversarial Networks
id: 2021-12-22t11-42-44z
tags: [uva, uva_dl1]
---

Generative Adversarial Networks (GANs) are a class of generative
[NNs](./2021-04-26t18-14-48z.md) that can be thought of as two networks
competing: a discriminator and a generator network.

The discriminator’s task is to determine whether a given input sample is real or
generated. It usually follows the architecture of typical classifier networks.

The generator’s task is to generate data that looks real enough to fool the
discriminator. Generators architectures are essentially inverted classifiers. In
the case of image generation, this usually consists in a deconvolutional neural
network.

The idea is that by training both networks in adversarial way, we can obtain a
high quality generator. Mathematically, we have a two-player minmax game, where
we wish to optimise the following loss function

$$
\min_{G} \max_{D} \mathbb{E}_{x \sim p_{\text {data }}}
\log D(x)+\mathbb{E}_{z \sim p_{G}} \log (1-D(G(z)).
$$

For better analysis, we can consider the goals of the generator and
discriminator individually. The generator wants to minimize the divergence
between the generated and data distributions $P_G$ and $P_{data}$. We write

$$
G^* = \arg \min_{G} Div(P_G, P_{data}).
$$

We know that the optimal discriminator is the one found when maximizing cross
entropy:

$$
D^* = \arg \max_{D} V(D, G) = \mathbb{E}_{x \sim P_{data}}[\log D(x)] +
\mathbb{E}_{x \sim P_G}[\log(1 - D(x))]
$$

I.e. the discriminator will be maximized when $P_G$ and $P_{data}$ diverge, so
we have found a measure of our divergence. We can therefore re-express our
optimal generator as

$$
G^* = \arg \min_{G} \max_{D} V(D, G).
$$

With this formulation, we find that the training process consists in the
following

1. Initialize the generator and the discriminator
2. For each training iteration:
   1. Fix the generator $G$ and update the discriminator $D$.
   2. Fix the discriminator $D$ and update the generator $G$.

For an optimal discriminator, it can be shown that the generator is minimizing
the Jensen Shannon (JS) divergence between true and generated distributions.
