---
date: 2022-10-21 17:23:28
title: (Iterative) Policy Evaluation (Prediction) in RL
id: 2022-10-21t17-23-28z
tags: [uva, uva_rl]
---

In [RL](./2022-10-20t15-15-55z.md), the _prediction_ problem deals with
computing the [value function](./2022-10-21t10-45-34z.md) $v_\pi$ for a given
[policy](./2022-10-21t10-19-39z.md) $\pi$. This is also referred to as _policy
evaluation_ in the context of [dynamic programming](./2022-10-21t17-13-39z.md).

When in the DP setting, the dynamics of the environment are known, and we can
perform _iterative policy evaluation_ by repeatedly applying the
[Bellman equation](./2022-10-21t12-59-49z.md) to approximate estimates of the
value function, eventually converging asymptotically to the true value function.

$$
v_{k+1}(s) = \sum_a \pi(a | s) \sum_{s', r} p(s', r | s, a) \left[
              r + \gamma v_k(s')
            \right].
$$

This is also known as an _expected update_, as each iteration of iterative
policy evaluation updates the value of every state to get a new approximate
value function.
