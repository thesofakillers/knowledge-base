---
date: 2021-12-19 23:11:04
title: RMSProp for SGD
id: 2021-12-19t23-11-04z
tags: [uva, uva_dl1]
---

This is a modification of [AdaGrad](./2021-12-19t23-03-21z.md) for non-convex
optimization. Here, we use an exponentially weighted average to accumulate
gradients instead of our squared sum.

Mathematically:

$$
\begin{aligned}
w_{t+1} &= w_t - v_t \\
v_t &= \frac{\eta}{\sqrt{r_t} + \varepsilon} g_t \\
r_t &= \alpha r_{t-1} + (1 - \alpha)g_t^2
\end{aligned}
$$

Where $\alpha$ is a decay hyperparameter.
