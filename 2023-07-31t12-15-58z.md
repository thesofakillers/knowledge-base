---
date: 2023-07-31 12:15:58
title: Two-phase training of CCLIPT may actually be beneficial
id: 2023-07-31t12-15-58z
tags: [msc_thesis]
---

The motivation behind
[CCLIPT was for closing the gap between textual and visual performance](./2023-07-11t11-38-00z.md),
and originally this was achieved by taking a pre-trained
[CLIPT](./2023-07-10t16-36-37z.md) checkpoint, freezing the visual encoder,
adding a contextualized head to the text encoder and training that.

In following runs, both textual and visual heads were trained contextualized,
from scratch together. We noticed
[unsatisfactory performance on neutral starting states](./2023-07-11t12-12-49z.md),
and were worried that due to the combination of shared context and contrastive
loss, CCLIPT was just learning to ignore the second half of its inputs and
simply relying on the shared context.

We tested this with [an ablation](./2023-07-11t12-32-21z.md), which we repeated
on [CCLIPT trained from scratch](./2023-07-12t13-29-12z.md). While this ablation
did not find evidence of relying purely on the start state, we were suspicious.
Another ablation, ran by Niklas, found that CCLIPT was indeed just relying on
the shared context.

To this end, training CCLIPT in two phases may actually be beneficial, as the
omitting the context in the first phase prevents the model from relying on it in
the second phase.

We refer to CCLIPT trained in this manner as CCLIPT-2P.
