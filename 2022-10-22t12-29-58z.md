---
date: 2022-10-22 12:29:58
title: Off-policy learning in RL
id: 2022-10-22t12-29-58z
tags: [uva, uva_rl]
---

[epsilon-soft policies](./2022-10-22t12-29-30z.md) address the
[exploration-exploitation dilemma](./2022-10-20t15-59-00z.md), but do so with a
compromise: they only converge to the _near-optimal_ policy rather than the
optimal one.

_Off-policy learning_ in [RL](./2022-10-20t15-15-55z.md) is a class of methods
where two (or more) policies are considered: a _target policy_ which we want to
learn about, and a _behaviour policy_, which we use to generate experience. This
is unlike _on-policy learning_ where we use the generate experience from the
same policy we are learning about (updating).

Off-policy methods can be considered as a general case of on-policy methods,
where the behaviour policy is the same as the target policy.

Off-policy learning requires [coverage](./2022-10-22t13-15-24z.md) and often
some form of [importance sampling](./2022-10-22t13-16-08z.md)
