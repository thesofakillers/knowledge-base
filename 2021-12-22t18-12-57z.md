---
date: 2021-12-22 18:12:57
title: Variational Inference and ELBO
id: 2021-12-22t18-12-57z
tags: [uva, uva_dl1]
---

Variational inference is a method for approximating the posterior necessary for
[training latent variable models](./2021-12-22t17-24-16z.md).

Introducing mathematical definitions, we approximate the exact posterior
$p_\theta(z|x)$ with an approximate posterior $q\_\phi(z|x)$, referred to as the
_variational posterior_.

We can use any distribution $q_\phi(z)$ as long as

- we can sample from it
- we can compute its gradient.

What this variational posterior provides is a lower bound on the marginal
log-likelihood necessary for obtaining our true posterior through
[Jensen's inequality](./2021-12-22t18-43-44z.md):

$$
\log p_\theta(x) \geq
\mathbb{E}_{q_\phi(z)}\left[\log\frac{p_\theta(x,z)}{q_\phi(z)}\right].
$$

The RHS, which we recognize as [KL divergence](./2021-12-22t18-20-26z.md), is
known as the _variational lower bound_. If design our variational posterior as
$q_\phi(z)$, we obtain the evidence lower bound (ELBO):

$$
\mathcal{L}_{\theta, \phi}(x) =
\mathbb{E}_{q_\phi(z)}\left[\log\frac{p_\theta(x,z)}{q_\phi(z|x)}\right].
$$

What all this allows us to do is to optimize (maximize) the ELBO rather than of
the (intractable) likelihood $p_\theta(x)$. The higher our ELBO the smaller the
difference between our variational posterior and our true posterior.

Remarkably, our (tractable) ELBO can be expressed in terms of two intractable
quantities:

$$
\mathcal{L}_{\theta, \phi}(x) =
\log p_\theta(x) - KL(q_\phi(z|x)||p_\theta(z|x))
$$
