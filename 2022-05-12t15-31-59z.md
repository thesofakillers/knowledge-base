---
date: 2022-05-12 15:31:59
title: The T5 Language Model
id: 2022-05-12t15-31-59z
---

T5 ("tee-five") [@raffel_exploring_2020] is a large text-to-text
encoder-decoder transformer pre-trained on a large, cleaned version of Common
Crawl (C4). The model is designed in a way such that it can be easily fine-tuned
for transfer on a variety of downstream tasks, reformatted as text-to-text
problems.

T5 by default refers to its prefix-variant, where fully-visible masking is used
during the prefix portion of the sequence. Typically, decoder-based
[LM](./2021-12-20t11-06-56z.md)'s make use of fully _causal_ masking. This
causes the model's representation of a prefix state to only depend on prior
entries of the prefix, which can be seen as a drawback.
