---
date: 2021-12-19 16:50:54
title: Pointwise Mutual Information (PMI)
id: 2021-12-19t16-50-54z
tags: [uva, uva_nlp1]
---

Pointwise Mutual Information (PMI) is a measure of how often two events $x$ and
$y$ co-occur compared to an a-priori estimate assuming independence between the
two events. It is used in [NLP](./2021-12-20t10-52-27z.md) in the context (no
pun intended) of [distributional semantics](./2021-12-19t16-47-22z.md) to weigh
a target word $w$'s [context](./2021-12-20t17-53-11z.md) $c$. Mathematically, it
is defined as

$$
\text{PMI}(w, c) = \log_2{\frac{P(w,c)}{P(w)P(c)}}.
$$

We can compute these probabilities by counting. Here we define $C(T)$ as the
total count of words in our corpus, $C(x, y)$ as the count of co-occurrences
(i.e. word $x$ in context $y$) of $x$ and $y$ and $C(z)$ the count of $z$
independent of its context or target word. With these definitions, our
probabilities can be computed with

$$
P(w, c) = \frac{C(w,c)}{C(T)}
$$

$$
P(w) = \frac{C(w)}{C(T)}
$$

$$
P(c) = \frac{C(c)}{C(T)}
$$
