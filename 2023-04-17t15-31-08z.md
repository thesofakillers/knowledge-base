---
date: 2023-04-17 15:31:08
title: Multi-modal approaches are the typical solution to grounding in LISDM
id: 2023-04-17t15-31-08z
tags: [msc_thesis]
---

At least as of 2023, it seems that leveraging
[multi-modal approaches](./2023-04-17t15-46-34z.md) is the go-to paradigm for
addressing the
[grounding problem in language-informed sequential decision making](./2023-04-17t11-15-58z.md).

This typically looks like making use of
[vision-language encoders](./2023-04-17t11-14-27z.md), with the
environment represented visually through images in pixel space.

The visual component of the multi-modal is responsible for "understanding" the
environment, while the language/textual component is responsible for
"understanding" language inputs (instructions, feedback, constraints, etc.).
Because these multi-modal models are typically trained such that the
representations from each modality are in the same semantic space, we
essentially have complementarily grounded representations of vision and language
between language and environment.

There are of course other ways to address the grounding problem, such as
bootstrapping [@watkins_teachable_2021], but generally it seems like the field
is dominated by multi-modality

## References
