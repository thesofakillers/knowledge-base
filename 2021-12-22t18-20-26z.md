---
date: 2021-12-22 18:20:26
title: Kullbackâ€“Leibler divergence
id: 2021-12-22t18-20-26z
tags: [uva, uva_dl1]
---

In [statistics](./2020-09-14t14-24-41z.md), the Kullback-Leibler divergence (KL)
is a metric measuring how one probability distribution $q$ is different from
another probability distribution $p$. Mathematically:

$$
D_{KL}(q || p) = \mathbb{E}_q[\log\frac{q}{p}]
$$

KL divergence is

- non-negative $D_{KL}(q || p) \geq 0$
- 0 only when the distributions are equal
- not symmetric $D_{KL}(q || p) \neq D_{KL}(p || q)$
