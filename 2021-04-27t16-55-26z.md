---
date: 2021-04-27 16:55:26
title: Backpropagation
id: 2021-04-27t16-55-26z
---

Backpropagation (also known as back-propagation or backprop) is an algorithm for
_efficiently_ computing the [gradient](./2021-04-27t18-05-20z.md) of a neural
network's [loss function](./2021-04-26t19-23-41z.md) with respect to the
network's [weights and biases](./2021-04-26t15-11-38z.md) so to successfully
perform [gradient descent](./2021-04-27t16-51-42z.md).

In fact, backpropagation is nothing more than an
[automatic differentiation](./2020-10-08t15-20-39z.md) technique, particularly
[reverse-mode AD](./2020-10-08t15-31-20z.md), applied to the network's loss
function.

Mathematically, backpropagation computes

$$
\frac{\partial C}{\partial w_{jk}^{(l)}}=
\frac{1}{n}\sum_{i=0}^{n-1}\frac{\partial C_i}{\partial w_{jk}^{(l)}},
$$

where $C_i$ is the loss function evaluated for a given training sample $y_i$ of
$n$ training samples for the weights $w_{jk}^{(l)}$ of the network connecting
the $k$th neuron in layer $l-1$ to the $jth$ neuron of layer $l$. The result is
a single component of the gradient of the cost function $\nabla C$. The same
process is repeated for each of the weights and biases of the network, obtaining
the full vector $\nabla C$.

By recalling the mechanisms of AD and in particular reverse-mode AD, we
understand that it does this by evaluating the chain rule from the outside
inwards. So if the cost function is defined to be (e.g. the squared error)

$$
C_i = (a^{(l)} - y_i)^2,
$$

where

$$
a_j^{(l)} = \sigma(z_j^{(l)})
$$

where $\sigma$ is some non-linear activation function and

$$
z_j^{(l)} = \dots + w_{jk}^{(l)} a_k^{(l-1)} + b_k^{(l)} + \dots,
$$

then in applying the chain rule we have

$$
\frac{\partial C_i}{\partial w_{jk}^{(l)}} = {
  \frac{\partial z_j^{(l)}}{\partial w_{jk}^{(l)}}
  \frac{\partial a_j^{(l)}}{\partial z_j^{(l)}}
  \frac{\partial C_i}{\partial a_j^{(l)}}
},
$$

whose components boil down to primitives handled by AD.

---

- [Amazing 3b1b video](https://www.youtube.com/watch?v=tIeHLnjs5U8)
- [blogpost tying backprop to AD](https://www.giulianomega.com/post/2018-05-04-backprop/#sec:fw-autodiff)
