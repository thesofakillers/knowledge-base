---
date: 2021-04-23 16:08:52
title: Vanishing/Exploding Gradients in Gradient-Based Learning Methods
id: 2021-04-23t16-08-52z
---

The vanishing gradient problem is a phenomenon which sometimes occurs during the
[training](./2021-04-26t19-23-41z.md) of
[neural networks](./2021-04-26t18-14-48z.md) where the
[gradient](./2021-04-27t18-05-20z.md) of the cost function with respect to the
[weights and biases](./2021-04-26t15-11-38z) in early
[layers](./2021-04-26t18-54-00z.md) of the network becomes increasingly small
until it vanishes.

This is problematic because a vanishing gradient for a particular weight means
vanishingly smaller changes to the activations tied by that weight. If this
happens early in the training process, this could lead to the model's
performance to stagnate, as its neurons practically become frozen.

The issue tends to be caused by the propagation of small values in the
components of chain rule evaluations as performed with
[backpropagation](./2021-04-27t16-55-26z.md). This is why the issue is most
evident at early layers of the network, where the chain rule for those weights
are comprised of more components, and therefore their product has a higher
chance of vanishing.

The opposite issue, that of exploding gradients, has the same origin, but rather
than being caused by the propagation of small values of the components of chain
rule evaluations, it is caused by the propagation of large values in the
components of these evaluations.

As opposed to freezing the affected neurons, exploding gradients essentially
cause the neurons to become overly sensitive, never properly settling at their
optimal values and instead oscillating sporadically.
